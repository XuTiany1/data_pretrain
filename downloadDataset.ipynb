{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10cd20da",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6730e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyixu/Documents/github_projects/data_pretrain/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datatrove.pipeline.readers import ParquetReader\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datatrove.pipeline.readers import ParquetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7399b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual Env Name:  /Users/tianyixu/Documents/github_projects/data_pretrain/venv\n"
     ]
    }
   ],
   "source": [
    "print(\"Virtual Env Name: \", os.environ.get(\"VIRTUAL_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da9d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the names of all languages\n",
    "\n",
    "african_language_list = [\n",
    "    'aeb_Arab',\n",
    "    'afr_Latn',\n",
    "    'aka_Latn',\n",
    "    'amh_Ethi',\n",
    "    'ary_Arab',\n",
    "    'arz_Arab',\n",
    "    'bam_Latn',\n",
    "    'bem_Latn',\n",
    "    'cjk_Latn',\n",
    "    'dik_Latn',\n",
    "    'dyu_Latn',\n",
    "    'ewe_Latn',\n",
    "    'fon_Latn',\n",
    "    'fuv_Latn',\n",
    "    'gaz_Latn',\n",
    "    'hau_Latn',\n",
    "    'ibo_Latn',\n",
    "    'kab_Latn',\n",
    "    'kam_Latn',\n",
    "    'kbp_Latn',\n",
    "    'kea_Latn',\n",
    "    'kik_Latn',\n",
    "    'kin_Latn',\n",
    "    'kmb_Latn',\n",
    "    'knc_Arab',\n",
    "    'knc_Latn',\n",
    "    'kon_Latn',\n",
    "    'lin_Latn',\n",
    "    'lua_Latn',\n",
    "    'lug_Latn',\n",
    "    'luo_Latn',\n",
    "    'mos_Latn',\n",
    "    'nqo_Nkoo',\n",
    "    'nso_Latn',\n",
    "    'nus_Latn',\n",
    "    'nya_Latn',\n",
    "    'plt_Latn',\n",
    "    'run_Latn',\n",
    "    'sag_Latn',\n",
    "    'sna_Latn',\n",
    "    'som_Latn',\n",
    "    'sot_Latn',\n",
    "    'ssw_Latn',\n",
    "    'swh_Latn',\n",
    "    'taq_Latn',\n",
    "    'taq_Tfng',\n",
    "    'tir_Ethi',\n",
    "    'tsn_Latn',\n",
    "    'tso_Latn',\n",
    "    'tum_Latn',\n",
    "    'twi_Latn',\n",
    "    'tzm_Tfng',\n",
    "    'umb_Latn',\n",
    "    'wol_Latn',\n",
    "    'xho_Latn',\n",
    "    'yor_Latn',\n",
    "    'zul_Latn',\n",
    "]\n",
    "\n",
    "high_lang_list = {\n",
    "    'eng_Latn',\n",
    "    'fra_Latn',\n",
    "    'por_Latn',\n",
    "    'arb_Arab'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad70fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with Wura Dataset mapping\n",
    "\n",
    "# Wura dataset follows the flores 200 mapping\n",
    "flores_200_mapping = {\n",
    "'afr_Latn': 'afr',\n",
    "'amh_Ethi': 'amh',\n",
    "'arb_Arab': 'ara',\n",
    "'asm_Beng': 'asm',\n",
    "'ast_Latn': 'ast',\n",
    "'azj_Latn': 'azj',\n",
    "'arz_Arab': 'arz',\n",
    "'bel_Cyrl': 'bel',\n",
    "'ben_Beng': 'ben',\n",
    "'bos_Latn': 'bos',\n",
    "'bul_Cyrl': 'bul',\n",
    "'cat_Latn': 'cat',\n",
    "'ceb_Latn': 'ceb',\n",
    "'ces_Latn': 'ces',\n",
    "'ckb_Arab': 'ckb',\n",
    "'cym_Latn': 'cym',\n",
    "'dan_Latn': 'dan',\n",
    "'deu_Latn': 'deu',\n",
    "'ell_Grek': 'ell',\n",
    "'eng_Latn': 'eng',\n",
    "'est_Latn': 'est',\n",
    "'fin_Latn': 'fin',\n",
    "'fra_Latn': 'fra',\n",
    "'fuv_Latn': 'ful',\n",
    "'gaz_Latn': 'gaz',\n",
    "'gle_Latn': 'gle',\n",
    "'glg_Latn': 'glg',\n",
    "'guj_Gujr': 'guj',\n",
    "'hau_Latn': 'hau',\n",
    "'heb_Hebr': 'heb',\n",
    "'hin_Deva': 'hin',\n",
    "'hrv_Latn': 'hrv',\n",
    "'hun_Latn': 'hun',\n",
    "'hye_Armn': 'hye',\n",
    "'ibo_Latn': 'ibo',\n",
    "'ind_Latn': 'ind',\n",
    "'isl_Latn': 'isl',\n",
    "'ita_Latn': 'ita',\n",
    "'jav_Latn': 'jav',\n",
    "'jpn_Jpan': 'jpn',\n",
    "'kam_Latn': 'kam',\n",
    "'kan_Knda': 'kan',\n",
    "'kat_Geor': 'kat',\n",
    "'kaz_Cyrl': 'kaz',\n",
    "'khm_Khmr': 'khm',\n",
    "'kir_Cyrl': 'kir',\n",
    "'kin_Latn': 'kin',\n",
    "'kor_Hang': 'kor',\n",
    "'lao_Laoo': 'lao',\n",
    "'lij_Latn': 'Latvian',\n",
    "'lim_Latn': 'kea',\n",
    "'lin_Latn': 'lin',\n",
    "'lit_Latn': 'lit',\n",
    "'ltz_Latn': 'ltz',\n",
    "'lug_Latn': 'lug',\n",
    "'luo_Latn': 'luo',\n",
    "'lvs_Latn': 'lav',\n",
    "'mal_Mlym': 'mal',\n",
    "'mar_Deva': 'mar',\n",
    "'mkd_Cyrl': 'mkd',\n",
    "'mlt_Latn': 'mlt',\n",
    "'khk_Cyrl': 'mon',\n",
    "'mri_Latn': 'mri',\n",
    "'mya_Mymr': 'mya',\n",
    "'nld_Latn': 'nld',\n",
    "'nob_Latn': 'nob',\n",
    "'npi_Deva': 'npi',\n",
    "'nso_Latn': 'nso',\n",
    "'nya_Latn': 'nya',\n",
    "'oci_Latn': 'oci',\n",
    "'gaz_Latn': 'orm',\n",
    "'ory_Orya': 'ory',\n",
    "'pan_Guru': 'pan',\n",
    "'pes_Arab': 'fas',\n",
    "'pol_Latn': 'pol',\n",
    "'por_Latn': 'por',\n",
    "'pbt_Arab': 'pus',\n",
    "'plt_Latn': 'plt',\n",
    "'ron_Latn': 'ron',\n",
    "'rus_Cyrl': 'rus',\n",
    "'slk_Latn': 'slk',\n",
    "'sna_Latn': 'sna',\n",
    "'snd_Arab': 'snd',\n",
    "'som_Latn': 'som',\n",
    "'spa_Latn': 'spa',\n",
    "'srp_Cyrl': 'srp',\n",
    "'swc_Latn': 'swc',\n",
    "'swe_Latn': 'swe',\n",
    "'swh_Latn': 'swa',\n",
    "'tam_Taml': 'tam',\n",
    "'tel_Telu': 'tel',\n",
    "'tgk_Cyrl': 'tgk',\n",
    "'tir_Ethi': 'tir',\n",
    "'tgl_Latn': 'tgl',\n",
    "'tha_Thai': 'tha',\n",
    "'tur_Latn': 'tur',\n",
    "'ukr_Cyrl': 'ukr',\n",
    "'umb_Latn': 'umb',\n",
    "'urd_Arab': 'urd',\n",
    "'uzn_Latn': 'uzb',\n",
    "'vie_Latn': 'vie',\n",
    "'wol_Latn': 'wol',\n",
    "'xho_Latn': 'xho',\n",
    "'yor_Latn': 'yor',\n",
    "'zho_Hans': 'zho_simpl',\n",
    "'zho_Hant': 'zho_trad',\n",
    "'zsm_Latn': 'msa',\n",
    "'zul_Latn': 'zul'}\n",
    "\n",
    "\n",
    "# List of all wura languages,\n",
    "wura_langs = [\n",
    "    \"afr\", \"amh\", \"arz\", \"eng\", \"fra\", \"hau\", \"ibo\", \"kin\",\n",
    "    \"mlg\", \"nya\", \"orm\", \"por\", \"sna\", \"som\", \"sot\",\n",
    "    \"swa\", \"tir\", \"xho\", \"yor\", \"zul\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab18b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the madlad 400 mapping\n",
    "\n",
    "afri_madlad_langs = {\n",
    "    \"afr_Latn\": \"af\",\n",
    "    \"aka_Latn\": \"ak\",\n",
    "    \"amh_Ethi\": \"am\",\n",
    "    \"bam_Latn\": \"bm\",\n",
    "    \"dik_Latn\": \"din\",\n",
    "    \"dyu_Latn\": \"dyu\",\n",
    "    \"ewe_Latn\": \"ee\",\n",
    "    \"fon_Latn\": \"fon\",\n",
    "    \"fuv_Latn\": \"ff\",\n",
    "    \"gaz_Latn\": \"om\",   \n",
    "    \"hau_Latn\": \"ha\",  \n",
    "    \"ibo_Latn\": \"ig\",   \n",
    "    \"kbp_Latn\": \"kbp\",\n",
    "    \"kin_Latn\": \"rw\",\n",
    "    \"kmb_Latn\": \"kmb\",\n",
    "    \"kon_Latn\": \"kg\",\n",
    "    \"lin_Latn\": \"ln\",\n",
    "    \"lug_Latn\": \"lg\",\n",
    "    \"run_Latn\": \"rn\",\n",
    "    \"sag_Latn\": \"sg\",\n",
    "    \"sna_Latn\": \"sn\",\n",
    "    \"som_Latn\": \"so\",\n",
    "    \"sot_Latn\": \"st\",\n",
    "    \"ssw_Latn\": \"ss\",\n",
    "    \"swh_Latn\": \"sw\",\n",
    "    \"tir_Ethi\": \"ti\",\n",
    "    \"tsn_Latn\": \"tn\",\n",
    "    \"tso_Latn\": \"ts\",\n",
    "    \"tzm_Tfng\": \"ber\",\n",
    "    \"wol_Latn\": \"wo\",\n",
    "    \"xho_Latn\": \"xh\",\n",
    "    \"yor_Latn\": \"yo\",\n",
    "    \"zul_Latn\": \"zu\"\n",
    "}\n",
    "\n",
    "hr_madlad_langs = {\n",
    "    \"eng_Latn\": \"en\",\n",
    "    \"fra_Latn\": \"fr\",\n",
    "    \"por_Latn\": \"pt\",\n",
    "    \"arb_Arab\": \"ar\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b74ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGSM like mapping\n",
    "\n",
    "\n",
    "afri_mgsm_langs = {\n",
    "    \"amh_Ethi\": \"amh\",\n",
    "    \"ewe_Latn\": \"ewe\",\n",
    "    \"gaz_Latn\": \"orm\",  \n",
    "    \"hau_Latn\": \"hau\",\n",
    "    \"ibo_Latn\": \"ibo\",\n",
    "    \"kin_Latn\": \"kin\",\n",
    "    \"lin_Latn\": \"lin\",\n",
    "    \"lug_Latn\": \"lug\",\n",
    "    \"sna_Latn\": \"sna\",\n",
    "    \"swh_Latn\": \"swa\",\n",
    "    \"sot_Latn\": \"sot\",\n",
    "    \"twi_Latn\": \"twi\",\n",
    "    \"wol_Latn\": \"wol\",\n",
    "    \"xho_Latn\": \"xho\",\n",
    "    \"yor_Latn\": \"yor\",\n",
    "    \"zul_Latn\": \"zul\"\n",
    "}\n",
    "\n",
    "mgsm_langs = {\n",
    "    \"eng_Latn\": \"en\",\n",
    "    \"fra_Latn\": \"fr\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74289bc",
   "metadata": {},
   "source": [
    "# Data folder Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for each language in all languages\n",
    "for lang in african_language_list:\n",
    "    os.makedirs(f\"data/{lang}\", exist_ok=True)\n",
    "\n",
    "for lang in high_lang_list:\n",
    "    os.makedirs(f\"data/{lang}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58abb4a",
   "metadata": {},
   "source": [
    "# Fineweb 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8909d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: {'text': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'dump': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'file_path': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'language_score': Value(dtype='float64', id=None), 'language_script': Value(dtype='string', id=None), 'minhash_cluster_size': Value(dtype='int64', id=None), 'top_langs': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": \"hf://datasets/HuggingFaceFW/fineweb-2/data/por_Latn/train/000_00000.parquet\"},\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"Available columns:\", dataset.features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a081d",
   "metadata": {},
   "source": [
    "## Crafting African Language Set that is part of FW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f86161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:06.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:06.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: aeb_Arab\n",
      "Exists: afr_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:08.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing: aka_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:08.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: amh_Ethi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:09.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ary_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:10.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: arz_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:11.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: bam_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:11.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:11.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: bem_Latn\n",
      "Exists: cjk_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:11.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:11.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: dik_Latn\n",
      "Exists: dyu_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:12.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ewe_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:12.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: fon_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:13.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: fuv_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:13.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: gaz_Latn\n",
      " Missing: hau_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:14.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ibo_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:15.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:15.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kab_Latn\n",
      "Exists: kam_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:15.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kbp_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:15.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kea_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:16.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kik_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:16.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kin_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:16.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: kmb_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:17.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: knc_Arab\n",
      "Exists: knc_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:17.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing: kon_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:17.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:18.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: lin_Latn\n",
      "Exists: lua_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:18.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: lug_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:18.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: luo_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:19.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:19.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: mos_Latn\n",
      "Exists: nqo_Nkoo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:19.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:19.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: nso_Latn\n",
      "Exists: nus_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:20.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: nya_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:20.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: plt_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:21.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: run_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:21.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: sag_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:21.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: sna_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:22.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: som_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:22.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: sot_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:23.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: ssw_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:23.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: swh_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:24.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:24.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: taq_Latn\n",
      "Exists: taq_Tfng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:24.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: tir_Ethi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:25.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: tsn_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:25.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:25.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: tso_Latn\n",
      "Exists: tum_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:25.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: twi_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:26.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n",
      "\u001b[32m2025-05-12 12:05:26.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: tzm_Tfng\n",
      "Exists: umb_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:26.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: wol_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:27.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: xho_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 12:05:27.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: yor_Latn\n",
      "Exists: zul_Latn\n",
      "\n",
      "Available languages: ['aeb_Arab', 'afr_Latn', 'amh_Ethi', 'ary_Arab', 'arz_Arab', 'bam_Latn', 'bem_Latn', 'cjk_Latn', 'dik_Latn', 'dyu_Latn', 'ewe_Latn', 'fon_Latn', 'fuv_Latn', 'gaz_Latn', 'ibo_Latn', 'kab_Latn', 'kam_Latn', 'kbp_Latn', 'kea_Latn', 'kik_Latn', 'kin_Latn', 'kmb_Latn', 'knc_Arab', 'knc_Latn', 'lin_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'mos_Latn', 'nqo_Nkoo', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'plt_Latn', 'run_Latn', 'sag_Latn', 'sna_Latn', 'som_Latn', 'sot_Latn', 'ssw_Latn', 'swh_Latn', 'taq_Latn', 'taq_Tfng', 'tir_Ethi', 'tsn_Latn', 'tso_Latn', 'tum_Latn', 'twi_Latn', 'tzm_Tfng', 'umb_Latn', 'wol_Latn', 'xho_Latn', 'yor_Latn', 'zul_Latn']\n"
     ]
    }
   ],
   "source": [
    "from datatrove.pipeline.readers import ParquetReader\n",
    "\n",
    "afri_fw2_existing_langs = []\n",
    "\n",
    "for lang_code in african_language_list:\n",
    "    path = f\"hf://datasets/HuggingFaceFW/fineweb-2/data/{lang_code}/train\"\n",
    "    try:\n",
    "        reader = ParquetReader(path, limit=1)  # Try to load just 1 file\n",
    "        _ = next(reader())                     # Trigger read\n",
    "        afri_fw2_existing_langs.append(lang_code)\n",
    "        print(f\"Exists: {lang_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Missing: {lang_code}\")\n",
    "\n",
    "print(\"\\nAvailable languages:\", afri_fw2_existing_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2370dd",
   "metadata": {},
   "source": [
    "## Crafting High Resource Language Set that is part of FW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5386efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 11:50:25.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/116\u001b[0m\n",
      "\u001b[32m2025-05-15 11:50:28.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/60\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: fra_Latn\n",
      "Exists: por_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-15 11:50:29.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mReading input file 000_00000.parquet, 1/25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing: eng_Latn\n",
      "Exists: arb_Arab\n",
      "\n",
      "Available languages: ['fra_Latn', 'por_Latn', 'arb_Arab']\n"
     ]
    }
   ],
   "source": [
    "from datatrove.pipeline.readers import ParquetReader\n",
    "\n",
    "hr_fw2_existing_langs = []\n",
    "\n",
    "for lang_code in high_lang_list:\n",
    "    path = f\"hf://datasets/HuggingFaceFW/fineweb-2/data/{lang_code}/train\"\n",
    "    try:\n",
    "        reader = ParquetReader(path, limit=1)  # Try to load just 1 file\n",
    "        _ = next(reader())                     # Trigger read\n",
    "        hr_fw2_existing_langs.append(lang_code)\n",
    "        print(f\"Exists: {lang_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Missing: {lang_code}\")\n",
    "\n",
    "print(\"\\nAvailable languages:\", hr_fw2_existing_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c3760",
   "metadata": {},
   "source": [
    "## Downloading African Language Datasets for FW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e529b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_fw2_parquet(lang_code: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Download the FineWeb-2 train split for one language **unchanged**\n",
    "    (same .parquet files, no decoding) and store them under `save_dir/lang_code/`.\n",
    "    \"\"\"\n",
    "    remote = f\"hf://datasets/HuggingFaceFW/fineweb-2/data/{lang_code}/train\"\n",
    "    fs, _ = fsspec.core.url_to_fs(remote)          # fsspec handles the HF filesystem\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Download all parquet files there, because its african languages so I download all of them\n",
    "    for idx, path in enumerate(tqdm(fs.glob(f\"{remote}/*.parquet\"),\n",
    "                                    desc=f\"Downloading {lang_code}\"), start=1):\n",
    "        fname      = f\"{lang_code}_{idx:04d}_fw2.parquet\"     # File name structure: afr_Latn_0001.parquet, ...\n",
    "        local_path = os.path.join(save_dir, fname)\n",
    "        fs.get(path, local_path)  # Download the file as is\n",
    "\n",
    "\n",
    "# Loop through all languages and download the parquet files\n",
    "for lang_code in afri_fw2_existing_langs:\n",
    "    save_dir = f\"data/{lang_code}\"\n",
    "    download_fw2_parquet(lang_code, save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635dd97",
   "metadata": {},
   "source": [
    "## Downloading first 1BT for High Resource Languages in FW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544d563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fra_Latn: 648tokens [00:02, 293.52tokens/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra_Latn: saved 2 shards (~648 tokens) → debug/fra_Latn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "por_Latn: 505tokens [00:02, 231.27tokens/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "por_Latn: saved 2 shards (~505 tokens) → debug/por_Latn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "arb_Arab: 3069tokens [00:01, 1653.76tokens/s]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arb_Arab: saved 2 shards (~3,069 tokens) → debug/arb_Arab\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eng_Latn: 794tokens [00:01, 420.58tokens/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_Latn: saved 2 shards (~794 tokens) → debug/eng_Latn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_limited_token_dataset(\n",
    "    lang_code: str,\n",
    "    hf_token: str,\n",
    "    *,\n",
    "    # existing parameters\n",
    "    target_token_count: int = 100,\n",
    "    batch_size: int = 3,\n",
    "    tokenizer_name: str = \"google/gemma-3-1b-it\",\n",
    "    # new parameters to support both loaders\n",
    "    loader: str = \"parquet\",                        # or the HF repo id\n",
    "    loader_kwargs: dict = None,                     # e.g. {\"name\":\"sample-10BT\",\"split\":\"train\"}\n",
    "    suffix: str = \"fw2\",                            # file suffix for shards\n",
    "    data_root: str = \"hf://datasets/HuggingFaceFW/fineweb-2/data\"\n",
    "):\n",
    "    save_dir = os.path.join(\"debug\", lang_code)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                              use_auth_token=hf_token)\n",
    "\n",
    "    # pick your loader\n",
    "    if loader == \"parquet\":\n",
    "        ds = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files={\"train\": f\"{data_root}/{lang_code}/train/000_00000.parquet\"},\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "    else:\n",
    "        # pass your token here so the HF loader can authenticate and resolve\n",
    "        ds = load_dataset(\n",
    "            loader,\n",
    "            **(loader_kwargs or {}),\n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "    buffer, total_tokens, shard_id = [], 0, 1\n",
    "    pbar = tqdm(total=target_token_count, unit=\"tokens\", desc=lang_code)\n",
    "\n",
    "    for ex in ds:\n",
    "        text = ex.get(\"text\", \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        tokens = tokenizer(text, truncation=False, padding=False)[\"input_ids\"]\n",
    "        total_tokens += len(tokens)\n",
    "        pbar.update(len(tokens))\n",
    "        buffer.append({\"text\": text})\n",
    "\n",
    "        if len(buffer) >= batch_size or total_tokens >= target_token_count:\n",
    "            df = pd.DataFrame(buffer)\n",
    "            name = f\"{lang_code}_{shard_id:04d}_{suffix}.parquet\"\n",
    "            df.to_parquet(os.path.join(save_dir, name), index=False)\n",
    "            shard_id += 1\n",
    "            buffer = []\n",
    "\n",
    "        if total_tokens >= target_token_count:\n",
    "            break\n",
    "\n",
    "    if buffer:\n",
    "        df = pd.DataFrame(buffer)\n",
    "        name = f\"{lang_code}_{shard_id:04d}_{suffix}.parquet\"\n",
    "        df.to_parquet(os.path.join(save_dir, name), index=False)\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"{lang_code}: saved {shard_id} shards (~{total_tokens:,} tokens) → {save_dir}\\n\")\n",
    "\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# 1) fineweb-2\n",
    "for lang in hr_fw2_existing_langs:\n",
    "    download_limited_token_dataset(\n",
    "        lang_code=lang,\n",
    "        hf_token=HF_TOKEN,\n",
    "        loader=\"parquet\",\n",
    "        suffix=\"fw2\",\n",
    "        data_root=\"hf://datasets/HuggingFaceFW/fineweb-2/data\"\n",
    "    )\n",
    "\n",
    "# 2) fineweb-edu (English)\n",
    "download_limited_token_dataset(\n",
    "    lang_code=\"eng_Latn\",\n",
    "    hf_token=HF_TOKEN,\n",
    "    loader=\"HuggingFaceFW/fineweb-edu\",\n",
    "    loader_kwargs={\"name\":\"sample-10BT\",\"split\":\"train\"},\n",
    "    suffix=\"fwedu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ec442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def download_limited_token_dataset(\n",
    "    lang_code: str,\n",
    "    hf_token: str,\n",
    "    target_token_count: int = 1_000_000_000,\n",
    "    batch_size: int = 100_000,\n",
    "    tokenizer_name: str = \"google/gemma-3-1b-it\",\n",
    "    data_root: str = \"hf://datasets/HuggingFaceFW/fineweb-2/data\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream the first `target_token_count` tokens of the fineweb-2 train split\n",
    "    for `lang_code`, tokenize, and save into parquet shards of size `batch_size`.\n",
    "    \"\"\"\n",
    "    save_dir = os.path.join(\"data\", lang_code)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                              use_auth_token=hf_token)\n",
    "    \n",
    "    # streaming load\n",
    "    ds = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files={\"train\": f\"{data_root}/{lang_code}/train/000_00000.parquet\"},\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    buffer = []\n",
    "    total_tokens = 0\n",
    "    shard_id = 1\n",
    "    pbar = tqdm(total=target_token_count, unit=\"tokens\",\n",
    "                desc=f\"{lang_code}\")\n",
    "    \n",
    "    # Iterate over dataset and tokenize \n",
    "    for example in ds:\n",
    "        text = example.get(\"text\", \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Tokenize the text only\n",
    "        input_ids = tokenizer(text, truncation=False, padding=False)[\"input_ids\"]\n",
    "        token_count = len(input_ids)\n",
    "        \n",
    "        buffer.append({\"text\": text})\n",
    "        total_tokens += token_count\n",
    "        pbar.update(token_count)\n",
    "        \n",
    "        # time to flush a shard?\n",
    "        if len(buffer) >= batch_size or total_tokens >= target_token_count:\n",
    "            df = pd.DataFrame(buffer)\n",
    "            shard_name = f\"{lang_code}_{shard_id:04d}_fw2.parquet\"\n",
    "            df.to_parquet(os.path.join(save_dir, shard_name), index=False)\n",
    "            shard_id += 1\n",
    "            buffer = []\n",
    "        \n",
    "        if total_tokens >= target_token_count:\n",
    "            break\n",
    "    \n",
    "    # final partial shard\n",
    "    if buffer:\n",
    "        df = pd.DataFrame(buffer)\n",
    "        shard_name = f\"{lang_code}_{shard_id:04d}_fw2.parquet\"\n",
    "        df.to_parquet(os.path.join(save_dir, shard_name), index=False)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"{lang_code}: saved {shard_id} shards, ~{total_tokens:,} tokens → {save_dir}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "for lang in hr_fw2_existing_langs:\n",
    "    download_limited_token_dataset(lang_code=lang, hf_token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0f482",
   "metadata": {},
   "source": [
    "# Fineweb Edu for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c879c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse function for fineweb-2 (English “sample-10BT”):\n",
    "\n",
    "download_limited_token_dataset(\n",
    "    lang_code=\"eng_Latn\",\n",
    "    hf_token=HF_TOKEN,\n",
    "    loader=\"HuggingFaceFW/fineweb-edu\",\n",
    "    loader_kwargs={\"name\":\"sample-10BT\",\"split\":\"train\"},\n",
    "    suffix=\"fwedu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED TOKENIZER\n",
      "Loaded DATASET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000001887tokens [33:56, 491013.70tokens/s]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Saved 11 shards with ~1,000,001,887 tokens to: data/eng_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "TARGET_TOKEN_COUNT = 1_000_000_000  # 1 billion tokens\n",
    "BATCH_SIZE = 100_000\n",
    "save_dir = \"data/eng_Latn\"\n",
    "file_prefix = \"eng_Latn\"\n",
    "\n",
    "# Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"LOADED TOKENIZER\")\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "\n",
    "print(\"Loaded DATASET\")\n",
    "\n",
    "# Streaming loop\n",
    "buffer = []\n",
    "total_tokens = 0\n",
    "shard_id = 1\n",
    "pbar = tqdm(total=TARGET_TOKEN_COUNT, unit=\"tokens\")\n",
    "\n",
    "for example in dataset:\n",
    "    text = example.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    input_ids = tokenizer(text, truncation=False, padding=False)[\"input_ids\"]\n",
    "    token_count = len(input_ids)\n",
    "\n",
    "    buffer.append({\"text\": text})\n",
    "    total_tokens += token_count\n",
    "    pbar.update(token_count)\n",
    "\n",
    "    if len(buffer) >= BATCH_SIZE or total_tokens >= TARGET_TOKEN_COUNT:\n",
    "        # Save this shard to a parquet file\n",
    "        df = pd.DataFrame(buffer)\n",
    "        shard_name = f\"{file_prefix}_{shard_id:04d}_fw2.parquet\"\n",
    "        df.to_parquet(os.path.join(save_dir, shard_name), index=False)\n",
    "        shard_id += 1\n",
    "        buffer = []\n",
    "\n",
    "    if total_tokens >= TARGET_TOKEN_COUNT:\n",
    "        break\n",
    "\n",
    "# Save any remaining buffer\n",
    "if buffer:\n",
    "    df = pd.DataFrame(buffer)\n",
    "    shard_name = f\"{file_prefix}_{shard_id:04d}_fw2.parquet\"\n",
    "    df.to_parquet(os.path.join(save_dir, shard_name), index=False)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\n✅ Done. Saved {shard_id} shards with ~{total_tokens:,} tokens to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd97f50",
   "metadata": {},
   "source": [
    "# WURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fde7821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'afr_Latn': 'afr', 'amh_Ethi': 'amh', 'arz_Arab': 'arz', 'gaz_Latn': 'orm', 'hau_Latn': 'hau', 'ibo_Latn': 'ibo', 'kin_Latn': 'kin', 'nya_Latn': 'nya', 'sna_Latn': 'sna', 'som_Latn': 'som', 'swh_Latn': 'swa', 'tir_Ethi': 'tir', 'xho_Latn': 'xho', 'yor_Latn': 'yor', 'zul_Latn': 'zul'}\n",
      "Number of languages in afri_wura_lang: 15\n"
     ]
    }
   ],
   "source": [
    "# Create mapping of African language lable to Wura langauges\n",
    "afri_wura_lang = {\n",
    "    lang: flores_200_mapping[lang]\n",
    "    for lang in african_language_list\n",
    "    if lang in flores_200_mapping and flores_200_mapping[lang] in wura_langs\n",
    "}\n",
    "\n",
    "print(afri_wura_lang)\n",
    "# print length of afri_wura_lang\n",
    "print(f\"Number of languages in afri_wura_lang: {len(afri_wura_lang)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36f5b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'por_Latn': 'por', 'fra_Latn': 'fra', 'eng_Latn': 'eng'}\n"
     ]
    }
   ],
   "source": [
    "# High resource languages avaialbe in Wura\n",
    "\n",
    "hr_wura_lang = {\n",
    "    lang: flores_200_mapping[lang]\n",
    "    for lang in high_lang_list\n",
    "    if lang in flores_200_mapping and flores_200_mapping[lang] in wura_langs\n",
    "}\n",
    "\n",
    "print(hr_wura_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d44142",
   "metadata": {},
   "source": [
    "## Downloading Wura for African Languages, Passage Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54640499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WURA dataset for language: afr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2390884 examples [00:21, 109539.78 examples/s]                      \n",
      "Generating validation split: 265117 examples [00:02, 109467.13 examples/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/afr_Latn/afr_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390884/2390884 [00:41<00:00, 57943.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/afr_Latn/afr_wura.txt\n",
      "Loading WURA dataset for language: amh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 291026 examples [00:05, 54060.84 examples/s]                     \n",
      "Generating validation split: 32307 examples [00:00, 54342.70 examples/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/amh_Ethi/amh_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291026/291026 [00:08<00:00, 35334.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/amh_Ethi/amh_wura.txt\n",
      "Loading WURA dataset for language: arz\n",
      "Saving data to data/arz_Arab/arz_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1116034/1116034 [00:11<00:00, 93822.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/arz_Arab/arz_wura.txt\n",
      "Loading WURA dataset for language: orm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 37280 examples [00:00, 128892.96 examples/s]                    \n",
      "Generating validation split: 4005 examples [00:00, 135245.66 examples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/gaz_Latn/orm_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37280/37280 [00:00<00:00, 80271.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/gaz_Latn/orm_wura.txt\n",
      "Loading WURA dataset for language: hau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 565471 examples [00:04, 123259.05 examples/s]                     \n",
      "Generating validation split: 63067 examples [00:00, 120974.69 examples/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/hau_Latn/hau_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 565471/565471 [00:08<00:00, 64773.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/hau_Latn/hau_wura.txt\n",
      "Loading WURA dataset for language: ibo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 121421 examples [00:01, 96586.56 examples/s]                   \n",
      "Generating validation split: 13899 examples [00:00, 77222.26 examples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/ibo_Latn/ibo_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121421/121421 [00:01<00:00, 65795.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/ibo_Latn/ibo_wura.txt\n",
      "Loading WURA dataset for language: kin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 120301 examples [00:00, 124077.06 examples/s]                   \n",
      "Generating validation split: 6902 examples [00:00, 131061.91 examples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/kin_Latn/kin_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120301/120301 [00:01<00:00, 78872.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/kin_Latn/kin_wura.txt\n",
      "Loading WURA dataset for language: nya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 150016 examples [00:01, 113182.90 examples/s]                   \n",
      "Generating validation split: 16880 examples [00:00, 115140.06 examples/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/nya_Latn/nya_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150016/150016 [00:01<00:00, 75460.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/nya_Latn/nya_wura.txt\n",
      "Loading WURA dataset for language: sna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 141559 examples [00:01, 119713.74 examples/s]                   \n",
      "Generating validation split: 16126 examples [00:00, 112140.36 examples/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/sna_Latn/sna_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141559/141559 [00:01<00:00, 78166.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/sna_Latn/sna_wura.txt\n",
      "Loading WURA dataset for language: som\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1235959 examples [00:09, 134330.05 examples/s]                    \n",
      "Generating validation split: 137938 examples [00:00, 139105.92 examples/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/som_Latn/som_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1235959/1235959 [00:18<00:00, 67298.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/som_Latn/som_wura.txt\n",
      "Loading WURA dataset for language: swa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1801101 examples [00:14, 122225.82 examples/s]                      \n",
      "Generating validation split: 200345 examples [00:01, 121385.67 examples/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/swh_Latn/swa_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1801101/1801101 [00:27<00:00, 64774.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/swh_Latn/swa_wura.txt\n",
      "Loading WURA dataset for language: tir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 9807 examples [00:00, 52096.99 examples/s]                   \n",
      "Generating validation split: 1084 examples [00:00, 56507.90 examples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/tir_Ethi/tir_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9807/9807 [00:00<00:00, 31060.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/tir_Ethi/tir_wura.txt\n",
      "Loading WURA dataset for language: xho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 69713 examples [00:00, 114813.28 examples/s]                    \n",
      "Generating validation split: 7846 examples [00:00, 101720.80 examples/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/xho_Latn/xho_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69713/69713 [00:00<00:00, 76505.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/xho_Latn/xho_wura.txt\n",
      "Loading WURA dataset for language: yor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 141321 examples [00:01, 98364.38 examples/s]                   \n",
      "Generating validation split: 15612 examples [00:00, 104066.20 examples/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/yor_Latn/yor_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141321/141321 [00:02<00:00, 66708.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/yor_Latn/yor_wura.txt\n",
      "Loading WURA dataset for language: zul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 166370 examples [00:01, 121777.41 examples/s]                   \n",
      "Generating validation split: 18289 examples [00:00, 115465.86 examples/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/zul_Latn/zul_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166370/166370 [00:02<00:00, 80169.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/zul_Latn/zul_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download African Languages for Wura\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wura_data(lang_code: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Download the WURA dataset (PASSAGE LEVEL) and save it as a text file\n",
    "    \n",
    "    Args:\n",
    "        lang_code: Language code for the dataset (e.g., 'hau', 'ibo', 'yor')\n",
    "        save_dir: Directory to save the downloaded dataset\n",
    "    \"\"\"\n",
    "    # Create the save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Output file path\n",
    "    output_file = os.path.join(save_dir, f\"{lang_code}_wura.txt\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(f\"Loading WURA dataset for language: {lang_code}\")\n",
    "    data = load_dataset(\"castorini/wura\", lang_code, level=\"passage\", verification_mode=\"no_checks\")\n",
    "    \n",
    "    # Write the text data to file\n",
    "    print(f\"Saving data to {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in tqdm(data['train']):\n",
    "            f.write(item['text'] + '\\n')\n",
    "    \n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "\n",
    "for lang_code, wura_code in afri_wura_lang.items():\n",
    "    save_dir = f\"data/{lang_code}\"\n",
    "    download_wura_data(wura_code, save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07538a3d",
   "metadata": {},
   "source": [
    "## Downloading WURA for High Resource langauges, Passage Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3138267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WURA dataset for language: fra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2220759 examples [00:20, 106846.40 examples/s]                      \n",
      "Generating validation split: 246611 examples [00:02, 105122.77 examples/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/fra_Latn/fra_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2220759/2220759 [00:35<00:00, 63007.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/fra_Latn/fra_wura.txt\n",
      "Loading WURA dataset for language: por\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1548167 examples [00:12, 123995.43 examples/s]                      \n",
      "Generating validation split: 173578 examples [00:01, 111056.08 examples/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/por_Latn/por_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1548167/1548167 [00:23<00:00, 67271.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/por_Latn/por_wura.txt\n",
      "Loading WURA dataset for language: eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2336199 examples [00:18, 127262.61 examples/s]                      \n",
      "Generating validation split: 260463 examples [00:01, 130732.86 examples/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/eng_Latn/eng_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2336199/2336199 [00:35<00:00, 65527.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/eng_Latn/eng_wura.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download African Languages for Wura\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wura_data(lang_code: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Download the WURA dataset (PASSAGE LEVEL) and save it as a text file\n",
    "    \n",
    "    Args:\n",
    "        lang_code: Language code for the dataset (e.g., 'hau', 'ibo', 'yor')\n",
    "        save_dir: Directory to save the downloaded dataset\n",
    "    \"\"\"\n",
    "    # Create the save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Output file path\n",
    "    output_file = os.path.join(save_dir, f\"{lang_code}_wura.txt\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(f\"Loading WURA dataset for language: {lang_code}\")\n",
    "    data = load_dataset(\"castorini/wura\", lang_code, level=\"passage\", verification_mode=\"no_checks\")\n",
    "    \n",
    "    # Write the text data to file\n",
    "    print(f\"Saving data to {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in tqdm(data['train']):\n",
    "            f.write(item['text'] + '\\n')\n",
    "    \n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "\n",
    "for lang_code, wura_code in hr_wura_lang.items():\n",
    "    save_dir = f\"data/{lang_code}\"\n",
    "    download_wura_data(wura_code, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ee0edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text']\n",
      "{'id': '0', 'text': \"AFCON 2019: Ghanaians attack referee Victor Gomes over Black Stars loss to Tunisia - The Black Stars have been eliminated from the 2019 AFCON in the round of 16 after being defeated on penalties - The match which ended 1-1 after extra time saw a first half goal from team captain, Dede Ayew, being disallowed by the referee Victor Gomes - Ghanaians who were incensed by the decision took to social media to insult the South African match official Ghana's Black Stars have been eliminated from the 2019 African Cup of Nations following their defeat to the Carthage Eagles of Tunisia. After playing out a 1-1 draw at the end of extra time, the fate of the Black Stars was to be decided by penalties. While Tunisians scored all of their five kicks, second-half substitute Caleb Ekuban missed for Ghana, leaving the penalties scoreline 5-4 in favour of Tunisia. READ ALSO: Gernot Rohr says Onyekuru, Osimhen not experienced enough to start for Super Eagles Ghanaians, who were heartbroken by the results, took to social media to vent their anger on the referee of the match, Victor Gomes. A check on Gomes' Instagram page revealed that his latest post had been flooded with over 2000 comments blasting him. The insults were so much that Gomes had to delete that particular photo and make his account private. Even with that, many people are still insulting him under other posts which have his picture. The anger against the South African referee was because he disallowed a goal that should have being a decisive factor for Ghana victory. Captain the Dede Ayew back-heeled the ball into the net after a cross from the right following a pass laid by Thomas Partey.. While explaining to the Ghanaian players why he disallowed the goal, referee Gomes gesticulated that the ball had hit Partey's hand before his pass. But TV footages clearly showed the ball hit the Atletico Madrid midfielder's mouth and not his hand, leading many to conclude that the referee had robbed Ghana of victory. PAY ATTENTION: Install Pitch Football App for FREE to get the latest football News & Scores Meanwhile, Legit.ng had reported that Odion Ighalo was rewarded for his five-star performance against Cameroon as the Super Eagles qualified to play South Africa in the quarter finals. The Shanghai Shenhua striker scored a brace in the highly entertaining fixture that saw Nigeria come from behind to beat the Indomitable Lions 3-2 at the Alexander Stadium.\"}\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"castorini/wura\", 'eng', level=\"passage\", verification_mode=\"no_checks\")\n",
    "print(data['train'].column_names)\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff9cfb",
   "metadata": {},
   "source": [
    "# Download for document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9556e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'headline', 'content', 'category', 'url']\n",
      "{'id': '1', 'headline': ' \\r\\n                  Itura de! Iṣẹ afara-nla Kutọ pari, ṣugbọn awọn ara Akute n bẹ Dapọ Abiọdun\\n', 'content': 'Pẹlu iroyin to n tẹ wa lọwọ, o ṣee ṣe ki iṣẹ afara-nla Kutọ pari laipẹ jọjọ. Gomina Dapọ Abiọdun lo fọrọ naa sita pe laipẹ lawọn agbaṣẹṣe maa pari ise naa, koda o ni wọn ti n da ọda sori biriiji naa lọwọ yii.Bakan naa lo tun mẹnuba a pe gbogbo awọn ojuna to wa lagbegbe afara-nla Kutọ ni awọn agbaṣẹṣe naa n ṣe lọwọ.Iṣẹ biriiji Kutọ yii wa lara awọn iṣẹ ti ijọba Ibikunle Amosun bẹrẹ ṣugbọn ti wọn pa ti, ko too di pe ijọba Abiọdun tun iṣẹ naa bẹrẹ lọtun.Gomina si ti ṣeleri pe gbogbo awọn iṣẹ yooku bẹẹ lawọn maa ṣe agbeyẹwo rẹ.Lara awọn iṣẹ ojuna ti awọn araalu n pariwo pe ki Dapọ Abiọdun gbe yẹwo ni ọna to lọ lati Ijoko Ọta wa si Berger, l’Ekoo.', 'category': None, 'url': 'https://www.asejere.net/itura-de-i%e1%b9%a3e-afara-nla-kuto-pari-%e1%b9%a3ugbon-awon-ara-akute-n-be-dapo-abiodun/'}\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"castorini/wura\", \"yor\", level=\"document\", verification_mode=\"no_checks\")\n",
    "                    \n",
    "print(data['train'].column_names)\n",
    "print(data['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61a1838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text']\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"castorini/wura\", \"yor\", level=\"passage\", verification_mode=\"no_checks\")\n",
    "                    \n",
    "print(data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "425593aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WURA dataset for language: afr\n",
      "Writing transformed JSONL to data/afr_Latn/afr_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1042812/1042812 [00:58<00:00, 17705.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/afr_Latn/afr_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: amh\n",
      "Writing transformed JSONL to data/amh_Ethi/amh_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 135863/135863 [00:10<00:00, 13083.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/amh_Ethi/amh_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: arz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1455662/1455662 [00:20<00:00, 70315.97 examples/s]\n",
      "Generating validation split: 100%|██████████| 161740/161740 [00:02<00:00, 68592.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/arz_Arab/arz_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1455662/1455662 [00:43<00:00, 33800.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/arz_Arab/arz_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: orm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 20169/20169 [00:00<00:00, 43086.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 2241/2241 [00:00<00:00, 47427.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/gaz_Latn/orm_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 20169/20169 [00:00<00:00, 26367.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/gaz_Latn/orm_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: hau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 359881/359881 [00:08<00:00, 42264.26 examples/s]\n",
      "Generating validation split: 100%|██████████| 39986/39986 [00:00<00:00, 43639.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/hau_Latn/hau_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 359881/359881 [00:16<00:00, 21919.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/hau_Latn/hau_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: ibo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 51386/51386 [00:01<00:00, 31944.25 examples/s]\n",
      "Generating validation split: 100%|██████████| 5709/5709 [00:00<00:00, 31011.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/ibo_Latn/ibo_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 51386/51386 [00:02<00:00, 20679.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/ibo_Latn/ibo_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: kin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 97064/97064 [00:01<00:00, 50121.71 examples/s]\n",
      "Generating validation split: 100%|██████████| 5831/5831 [00:00<00:00, 54234.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/kin_Latn/kin_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 97064/97064 [00:03<00:00, 28018.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/kin_Latn/kin_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: nya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 39647/39647 [00:01<00:00, 23624.82 examples/s]\n",
      "Generating validation split: 100%|██████████| 4405/4405 [00:00<00:00, 13636.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/nya_Latn/nya_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 39647/39647 [00:03<00:00, 12018.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/nya_Latn/nya_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: sna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 60986/60986 [00:01<00:00, 39217.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 6776/6776 [00:00<00:00, 37269.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/sna_Latn/sna_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 60986/60986 [00:02<00:00, 23257.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/sna_Latn/sna_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: som\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 976484/976484 [00:19<00:00, 49044.24 examples/s]\n",
      "Generating validation split: 100%|██████████| 108498/108498 [00:02<00:00, 48988.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/som_Latn/som_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 976484/976484 [00:43<00:00, 22612.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/som_Latn/som_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: swa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1036254/1036254 [00:27<00:00, 37882.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 115139/115139 [00:02<00:00, 40600.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/swh_Latn/swa_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1036254/1036254 [00:53<00:00, 19518.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/swh_Latn/swa_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: tir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8240/8240 [00:00<00:00, 31747.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 915/915 [00:00<00:00, 30916.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/tir_Ethi/tir_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 8240/8240 [00:00<00:00, 23094.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/tir_Ethi/tir_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: xho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 23892/23892 [00:00<00:00, 32444.41 examples/s]\n",
      "Generating validation split: 100%|██████████| 2654/2654 [00:00<00:00, 33111.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/xho_Latn/xho_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 23892/23892 [00:01<00:00, 21538.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/xho_Latn/xho_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: yor\n",
      "Writing transformed JSONL to data/yor_Latn/yor_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 73473/73473 [00:03<00:00, 19566.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/yor_Latn/yor_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: zul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 65447/65447 [00:01<00:00, 34357.80 examples/s]\n",
      "Generating validation split: 100%|██████████| 7271/7271 [00:00<00:00, 36510.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/zul_Latn/zul_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 65447/65447 [00:03<00:00, 19023.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/zul_Latn/zul_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wura_data(lang_code: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Download the WURA dataset (Document LEVEL) and save it as a jsonl file,\n",
    "    concatenating `headline` + `content` into `text`, and putting everything\n",
    "    else under `hyperparam`.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    output_file = os.path.join(save_dir, f\"{lang_code}_wura_documentLevel.jsonl\")\n",
    "\n",
    "    print(f\"Loading WURA dataset for language: {lang_code}\")\n",
    "    data = load_dataset(\n",
    "        \"castorini/wura\",\n",
    "        lang_code,\n",
    "        level=\"document\",\n",
    "        verification_mode=\"no_checks\"\n",
    "    )\n",
    "\n",
    "    print(f\"Writing transformed JSONL to {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in tqdm(data['train'], desc=\"items\"):\n",
    "            # 1) safe-get + strip headline/content\n",
    "            headline = (item.get('headline') or \"\").strip()\n",
    "            content  = (item.get('content')  or \"\").strip()\n",
    "\n",
    "            # 2) concatenate and trim any extra whitespace\n",
    "            text = f\"{headline} {content}\".strip()\n",
    "\n",
    "            # 3) stash the rest under hyperparam\n",
    "            hyperparam = {\n",
    "                k: v\n",
    "                for k, v in item.items()\n",
    "                if k not in ('headline', 'content')\n",
    "            }\n",
    "\n",
    "            # 4) write one JSON line\n",
    "            out = {\n",
    "                \"text\": text,\n",
    "                \"hyperparam\": hyperparam\n",
    "            }\n",
    "            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Done → {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for lang_folder, wura_code in afri_wura_lang.items():\n",
    "        save_dir = os.path.join(\"data\", lang_folder)\n",
    "        download_wura_data(wura_code, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee70eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WURA dataset for language: por\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1089199/1089199 [00:22<00:00, 49116.56 examples/s]\n",
      "Generating validation split: 100%|██████████| 121022/121022 [00:02<00:00, 49751.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/por_Latn/por_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1089199/1089199 [00:44<00:00, 24446.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/por_Latn/por_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: fra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1443177/1443177 [00:32<00:00, 44996.29 examples/s]\n",
      "Generating validation split: 100%|██████████| 160352/160352 [00:04<00:00, 33930.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/fra_Latn/fra_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1443177/1443177 [01:02<00:00, 23170.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/fra_Latn/fra_wura_documentLevel.jsonl\n",
      "Loading WURA dataset for language: eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1378555/1378555 [00:29<00:00, 47322.47 examples/s]\n",
      "Generating validation split: 100%|██████████| 153172/153172 [00:03<00:00, 47078.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed JSONL to data/eng_Latn/eng_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "items: 100%|██████████| 1378555/1378555 [01:01<00:00, 22485.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → data/eng_Latn/eng_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wura_data(lang_code: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Download the WURA dataset (Document LEVEL) and save it as a jsonl file,\n",
    "    concatenating `headline` + `content` into `text`, and putting everything\n",
    "    else under `hyperparam`.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    output_file = os.path.join(save_dir, f\"{lang_code}_wura_documentLevel.jsonl\")\n",
    "\n",
    "    print(f\"Loading WURA dataset for language: {lang_code}\")\n",
    "    data = load_dataset(\n",
    "        \"castorini/wura\",\n",
    "        lang_code,\n",
    "        level=\"document\",\n",
    "        verification_mode=\"no_checks\"\n",
    "    )\n",
    "\n",
    "    print(f\"Writing transformed JSONL to {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in tqdm(data['train'], desc=\"items\"):\n",
    "            # 1) safe-get + strip headline/content\n",
    "            headline = (item.get('headline') or \"\").strip()\n",
    "            content  = (item.get('content')  or \"\").strip()\n",
    "\n",
    "            # 2) concatenate and trim any extra whitespace\n",
    "            text = f\"{headline} {content}\".strip()\n",
    "\n",
    "            # 3) stash the rest under hyperparam\n",
    "            hyperparam = {\n",
    "                k: v\n",
    "                for k, v in item.items()\n",
    "                if k not in ('headline', 'content')\n",
    "            }\n",
    "\n",
    "            # 4) write one JSON line\n",
    "            out = {\n",
    "                \"text\": text,\n",
    "                \"hyperparam\": hyperparam\n",
    "            }\n",
    "            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Done → {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for lang_folder, wura_code in hr_wura_lang.items():\n",
    "        save_dir = os.path.join(\"data\", lang_folder)\n",
    "        download_wura_data(wura_code, save_dir)\n",
    "# Download Wura dataset for high resource languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d089b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'hyperparam']\n",
      "{'text': 'Polícia acusada de facilitar entrada de munícipes num posto de recenseamento eleitoral Um agente da PRM foi acusado, hoje, na cidade da Beira, de estar a interferir negativamente no processo de recenseamento eleitoral, ao facilitar a entrada de pessoas para serem inscritas em detrimento de dezenas de munícipes que estavam na fila desde a madrugada. A situação criou um tumulto que paralisou, por alguns instantes, o processo na capital de Sofala. Os munícipes da Beira manifestaram-se, na manhã desta segunda-feira, contra o agente da Polícia que, alegadamente, facilitou a entrada de sete pessoas para serem recenseadas, em detrimento dos que estavam na fila desde a madrugada e de outros que há vários dias não conseguem recensear-se na Escola Primária de Macombe, no bairro da Munhava. A comunidade insurgiu-se e um dos jovens do bairro foi detido. O jovem em causa foi levado para uma viatura da Polícia e, enquanto a nossa equipa de reportagem tentava colher o seu depoimento, os agentes conduziram a viatura para uma das esquadras da cidade da Beira. A comunidade local afirmou ainda que há sempre uma lista na posse dos brigadistas de pessoas para serem recenseadas ou os mesmos facilitam a entrada de pessoas que não estão na fila. Os munícipes entrevistados acrescentaram que, devido às alegadas listas das filas, diariamente, só são recenseadas três a cinco pessoas. O director do STAE, na cidade da Beira, afirmou que irá averiguar o caso e que, oportunamente, irá pronunciar-se.', 'hyperparam': {'id': '0', 'category': None, 'url': 'https://opais.co.mz/policia-acusada-de-facilitar-entrada-de-municipes-num-posto-de-recenseamento-eleitoral/'}}\n"
     ]
    }
   ],
   "source": [
    "# load your single JSONL as the “train” split\n",
    "data = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={ \"train\": \"data/por_Latn/por_wura_documentLevel.jsonl\" }\n",
    ")\n",
    "\n",
    "# inspect the columns\n",
    "print(data[\"train\"].column_names)\n",
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e67d82",
   "metadata": {},
   "source": [
    "# Madlad 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9838b5",
   "metadata": {},
   "source": [
    "## Madlad 400 for African Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd4af67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the madlad 400 mapping\n",
    "\n",
    "afri_madlad_langs = {\n",
    "    \"afr_Latn\": \"af\",\n",
    "    \"aka_Latn\": \"ak\",\n",
    "    \"amh_Ethi\": \"am\",\n",
    "    \"bam_Latn\": \"bm\",\n",
    "    \"dik_Latn\": \"din\",\n",
    "    \"dyu_Latn\": \"dyu\",\n",
    "    \"ewe_Latn\": \"ee\",\n",
    "    \"fon_Latn\": \"fon\",\n",
    "    \"fuv_Latn\": \"ff\",\n",
    "    \"gaz_Latn\": \"om\",   \n",
    "    \"hau_Latn\": \"ha\",  \n",
    "    \"ibo_Latn\": \"ig\",   \n",
    "    \"kbp_Latn\": \"kbp\",\n",
    "    \"kin_Latn\": \"rw\",\n",
    "    \"kmb_Latn\": \"kmb\",\n",
    "    \"kon_Latn\": \"kg\",\n",
    "    \"lin_Latn\": \"ln\",\n",
    "    \"lug_Latn\": \"lg\",\n",
    "    \"run_Latn\": \"rn\",\n",
    "    \"sag_Latn\": \"sg\",\n",
    "    \"sna_Latn\": \"sn\",\n",
    "    \"som_Latn\": \"so\",\n",
    "    \"sot_Latn\": \"st\",\n",
    "    \"ssw_Latn\": \"ss\",\n",
    "    \"swh_Latn\": \"sw\",\n",
    "    \"tir_Ethi\": \"ti\",\n",
    "    \"tsn_Latn\": \"tn\",\n",
    "    \"tso_Latn\": \"ts\",\n",
    "    \"tzm_Tfng\": \"ber\",\n",
    "    \"wol_Latn\": \"wo\",\n",
    "    \"xho_Latn\": \"xh\",\n",
    "    \"yor_Latn\": \"yo\",\n",
    "    \"zul_Latn\": \"zu\"\n",
    "}\n",
    "\n",
    "hr_madlad_langs = {\n",
    "    \"eng_Latn\": \"en\",\n",
    "    \"fra_Latn\": \"fr\",\n",
    "    \"por_Latn\": \"pt\",\n",
    "    \"arb_Arab\": \"ar\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "245614a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading all chunks for af ...\n",
      "Downloaded: af_clean_0000.jsonl.gz\n",
      "No more files after: af_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/afr_Latn/afr_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ak ...\n",
      "Downloaded: ak_clean_0000.jsonl.gz\n",
      "No more files after: ak_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/aka_Latn/aka_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for am ...\n",
      "Downloaded: am_clean_0000.jsonl.gz\n",
      "No more files after: am_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/amh_Ethi/amh_Ethi_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for bm ...\n",
      "Downloaded: bm_clean_0000.jsonl.gz\n",
      "No more files after: bm_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/bam_Latn/bam_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for din ...\n",
      "Downloaded: din_clean_0000.jsonl.gz\n",
      "No more files after: din_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/dik_Latn/dik_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for dyu ...\n",
      "Downloaded: dyu_clean_0000.jsonl.gz\n",
      "No more files after: dyu_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/dyu_Latn/dyu_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ee ...\n",
      "Downloaded: ee_clean_0000.jsonl.gz\n",
      "No more files after: ee_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/ewe_Latn/ewe_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for fon ...\n",
      "Downloaded: fon_clean_0000.jsonl.gz\n",
      "No more files after: fon_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/fon_Latn/fon_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ff ...\n",
      "Downloaded: ff_clean_0000.jsonl.gz\n",
      "No more files after: ff_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/fuv_Latn/fuv_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for om ...\n",
      "Downloaded: om_clean_0000.jsonl.gz\n",
      "No more files after: om_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/gaz_Latn/gaz_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ha ...\n",
      "Downloaded: ha_clean_0000.jsonl.gz\n",
      "No more files after: ha_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/hau_Latn/hau_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ig ...\n",
      "Downloaded: ig_clean_0000.jsonl.gz\n",
      "No more files after: ig_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/ibo_Latn/ibo_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for kbp ...\n",
      "Downloaded: kbp_clean_0000.jsonl.gz\n",
      "No more files after: kbp_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/kbp_Latn/kbp_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for rw ...\n",
      "Downloaded: rw_clean_0000.jsonl.gz\n",
      "No more files after: rw_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/kin_Latn/kin_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for kmb ...\n",
      "Downloaded: kmb_clean_0000.jsonl.gz\n",
      "No more files after: kmb_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/kmb_Latn/kmb_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for kg ...\n",
      "Downloaded: kg_clean_0000.jsonl.gz\n",
      "No more files after: kg_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/kon_Latn/kon_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ln ...\n",
      "Downloaded: ln_clean_0000.jsonl.gz\n",
      "No more files after: ln_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/lin_Latn/lin_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for lg ...\n",
      "Downloaded: lg_clean_0000.jsonl.gz\n",
      "No more files after: lg_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/lug_Latn/lug_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for rn ...\n",
      "Downloaded: rn_clean_0000.jsonl.gz\n",
      "No more files after: rn_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/run_Latn/run_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for sg ...\n",
      "Downloaded: sg_clean_0000.jsonl.gz\n",
      "No more files after: sg_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/sag_Latn/sag_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for sn ...\n",
      "Downloaded: sn_clean_0000.jsonl.gz\n",
      "No more files after: sn_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/sna_Latn/sna_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for so ...\n",
      "Downloaded: so_clean_0000.jsonl.gz\n",
      "No more files after: so_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/som_Latn/som_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for st ...\n",
      "Downloaded: st_clean_0000.jsonl.gz\n",
      "No more files after: st_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/sot_Latn/sot_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ss ...\n",
      "Downloaded: ss_clean_0000.jsonl.gz\n",
      "No more files after: ss_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/ssw_Latn/ssw_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for sw ...\n",
      "Downloaded: sw_clean_0000.jsonl.gz\n",
      "No more files after: sw_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/swh_Latn/swh_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ti ...\n",
      "Downloaded: ti_clean_0000.jsonl.gz\n",
      "No more files after: ti_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/tir_Ethi/tir_Ethi_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for tn ...\n",
      "Downloaded: tn_clean_0000.jsonl.gz\n",
      "No more files after: tn_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/tsn_Latn/tsn_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ts ...\n",
      "Downloaded: ts_clean_0000.jsonl.gz\n",
      "No more files after: ts_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/tso_Latn/tso_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for ber ...\n",
      "Downloaded: ber_clean_0000.jsonl.gz\n",
      "No more files after: ber_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/tzm_Tfng/tzm_Tfng_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for wo ...\n",
      "Downloaded: wo_clean_0000.jsonl.gz\n",
      "No more files after: wo_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/wol_Latn/wol_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for xh ...\n",
      "Downloaded: xh_clean_0000.jsonl.gz\n",
      "No more files after: xh_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/xho_Latn/xho_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for yo ...\n",
      "Downloaded: yo_clean_0000.jsonl.gz\n",
      "No more files after: yo_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/yor_Latn/yor_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n",
      "Downloading all chunks for zu ...\n",
      "Downloaded: zu_clean_0000.jsonl.gz\n",
      "No more files after: zu_clean_0001.jsonl.gz\n",
      "\n",
      "Final merged file saved to: data/zul_Latn/zul_Latn_ml400.jsonl\n",
      "🧹 Cleaned up temp files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def download_and_merge_madlad_clean_files(lang_code: str, output_filename: str, save_dir: str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://huggingface.co/datasets/allenai/madlad-400/resolve/main/data/{lang_code}/\"\n",
    "    i = 0\n",
    "    temp_dir = os.path.join(save_dir, \"temp_chunks\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Download all chunks\n",
    "    print(f\"Downloading all chunks for {lang_code} ...\")\n",
    "    while True:\n",
    "        filename = f\"{lang_code}_clean_{i:04d}.jsonl.gz\"\n",
    "        url = base_url + filename\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "            i += 1\n",
    "        else:\n",
    "            print(f\"No more files after: {filename}\")\n",
    "            break\n",
    "\n",
    "    # Step 2: Decompress and merge into single file\n",
    "    final_path = os.path.join(save_dir, output_filename)\n",
    "    with open(final_path, 'wb') as outfile:\n",
    "        for j in range(i):\n",
    "            part_file = os.path.join(temp_dir, f\"{lang_code}_clean_{j:04d}.jsonl.gz\")\n",
    "            with gzip.open(part_file, 'rb') as f_in:\n",
    "                shutil.copyfileobj(f_in, outfile)\n",
    "\n",
    "    print(f\"\\nFinal merged file saved to: {final_path}\")\n",
    "\n",
    "    # Step 3: Clean up\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(\"🧹 Cleaned up temp files.\")\n",
    "\n",
    "\n",
    "for lang_code, madlad_code in afri_madlad_langs.items():\n",
    "    download_and_merge_madlad_clean_files(madlad_code, f\"{lang_code}_ml400.jsonl\", f\"data/{lang_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80febfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['text']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to one of your merged JSONL files\n",
    "file_path = \"data/afr_Latn/afr_Latn_ml400.jsonl\"\n",
    "\n",
    "# Read the first non-empty line\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            sample = json.loads(line)\n",
    "            break\n",
    "\n",
    "# Print the keys (columns)\n",
    "print(\"Columns:\", list(sample.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391e7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading all chunks for en ...\n",
      "Downloaded: en_clean_0000.jsonl.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🧹 Cleaned up temp files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lang_code, madlad_code \u001b[38;5;129;01min\u001b[39;00m hr_madlad_langs.items():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mdownload_and_merge_madlad_clean_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmadlad_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang_code\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_ml400.jsonl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang_code\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mdownload_and_merge_madlad_clean_files\u001b[39m\u001b[34m(lang_code, output_filename, save_dir)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def download_and_merge_madlad_clean_files(lang_code: str, output_filename: str, save_dir: str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://huggingface.co/datasets/allenai/madlad-400/resolve/main/data/{lang_code}/\"\n",
    "    i = 0\n",
    "    temp_dir = os.path.join(save_dir, \"temp_chunks\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Download all chunks\n",
    "    print(f\"Downloading all chunks for {lang_code} ...\")\n",
    "    while True:\n",
    "        filename = f\"{lang_code}_clean_{i:04d}.jsonl.gz\"\n",
    "        url = base_url + filename\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "            i += 1\n",
    "        else:\n",
    "            print(f\"No more files after: {filename}\")\n",
    "            break\n",
    "\n",
    "    # Step 2: Decompress and merge into single file\n",
    "    final_path = os.path.join(save_dir, output_filename)\n",
    "    with open(final_path, 'wb') as outfile:\n",
    "        for j in range(i):\n",
    "            part_file = os.path.join(temp_dir, f\"{lang_code}_clean_{j:04d}.jsonl.gz\")\n",
    "            with gzip.open(part_file, 'rb') as f_in:\n",
    "                shutil.copyfileobj(f_in, outfile)\n",
    "\n",
    "    print(f\"\\nFinal merged file saved to: {final_path}\")\n",
    "\n",
    "    # Step 3: Clean up\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(\"🧹 Cleaned up temp files.\")\n",
    "\n",
    "\n",
    "for lang_code, madlad_code in hr_madlad_langs.items():\n",
    "    download_and_merge_madlad_clean_files(madlad_code, f\"{lang_code}_ml400.jsonl\", f\"data/{lang_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to one of your merged JSONL files\n",
    "file_path = \"data/afr_Latn/afr_Latn_ml400.jsonl\"\n",
    "\n",
    "# Read the first non-empty line\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            sample = json.loads(line)\n",
    "            break\n",
    "\n",
    "# Print the keys (columns)\n",
    "print(\"Columns:\", list(sample.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bff77",
   "metadata": {},
   "source": [
    "# Extra Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df775a0",
   "metadata": {},
   "source": [
    "## Tswana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e09653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Output folder & path\u001b[39;00m\n\u001b[32m      2\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33m./data/tsn_Latn\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mos\u001b[49m.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m output_path = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mtsn_Latn_extra.jsonl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the extra dataset\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Output folder & path\n",
    "output_dir = \"./data/tsn_Latn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"tsn_Latn_extra.jsonl\")\n",
    "\n",
    "# Load the extra dataset\n",
    "ds = load_dataset(\"OxxoCodes/Marothodi\", split=\"train\")\n",
    "\n",
    "# Print structure of first example to verify\n",
    "if len(ds) > 0:\n",
    "    print(\"Marothodi example structure:\", ds[0])\n",
    "\n",
    "# Check if the file exists and how many lines it has\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_lines = sum(1 for _ in f)\n",
    "\n",
    "    if existing_lines >= len(ds):\n",
    "        print(f\"{output_path} already contains all {existing_lines} examples, skipping.\")\n",
    "    else:\n",
    "        print(f\"{output_path} has {existing_lines}/{len(ds)} examples — appending missing...\")\n",
    "        with open(output_path, \"a\", encoding=\"utf-8\") as out_f:\n",
    "            for i, example in enumerate(ds):\n",
    "                if i < existing_lines:\n",
    "                    continue\n",
    "                \n",
    "                # Create standardized format with \"text\" field\n",
    "                if \"text\" in example:\n",
    "                    json.dump({\"text\": example[\"text\"]}, out_f)\n",
    "                elif \"sentence\" in example:  # Assuming the dataset might have a \"sentence\" field\n",
    "                    json.dump({\"text\": example[\"sentence\"]}, out_f)\n",
    "                else:\n",
    "                    # If no clear text field, identify the appropriate field based on dataset structure\n",
    "                    # For example, concatenate multiple fields or use a specific field:\n",
    "                    # You may need to adjust this based on the actual structure\n",
    "                    content = str(example)\n",
    "                    json.dump({\"text\": content}, out_f)\n",
    "                \n",
    "                out_f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"Appended {len(ds) - existing_lines} new examples to {output_path}\")\n",
    "else:\n",
    "    # File does not exist; write all from scratch\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for example in ds:\n",
    "            # Create standardized format with \"text\" field\n",
    "            if \"text\" in example:\n",
    "                json.dump({\"text\": example[\"text\"]}, out_f)\n",
    "            elif \"sentence\" in example:  # Assuming the dataset might have a \"sentence\" field\n",
    "                json.dump({\"text\": example[\"sentence\"]}, out_f)\n",
    "            else:\n",
    "                # If no clear text field, identify the appropriate field based on dataset structure\n",
    "                content = str(example)\n",
    "                json.dump({\"text\": content}, out_f)\n",
    "            \n",
    "            out_f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Wrote all {len(ds)} examples to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed9c92a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'source', 'source-category']\n",
      "{'text': 'Ka goo Ruthe aya le Naomi Betlehema nageng ya Israele.', 'source': 'https://downloads.wortschatz-leipzig.de/corpora/tsn_community_2017.tar.gz', 'source-category': 'tsn_community_2017'}\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"OxxoCodes/Marothodi\", split=\"train\")\n",
    "print(ds.column_names)\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd94f7",
   "metadata": {},
   "source": [
    "# Convert to LLaMMa Factory Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cc6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:44:28,501 - INFO - Found 61 language directories\n",
      "2025-05-14 14:44:28,501 - INFO - Processing language: fon_Latn\n",
      "2025-05-14 14:44:28,502 - INFO - Processing file: fon_Latn_ml400.jsonl\n",
      "2025-05-14 14:44:28,502 - INFO - Skipping fon_Latn_ml400.jsonl: output already at preprocessed_data/fon_Latn/fon_Latn_ml400.parquet\n",
      "2025-05-14 14:44:28,503 - INFO - Processing file: fon_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:44:28,503 - INFO - Skipping fon_Latn_0001_fw2.parquet: output already at preprocessed_data/fon_Latn/fon_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:44:28,504 - INFO - Processing language: hau_Latn\n",
      "2025-05-14 14:44:28,505 - INFO - Processing file: hau_wura.txt\n",
      "2025-05-14 14:44:28,506 - INFO - Processing file: hau_Latn_ml400.jsonl\n",
      "2025-05-14 14:44:28,506 - INFO - Processing file: hau_wura_documentLevel.jsonl\n",
      "2025-05-14 14:44:40,593 - ERROR - Error streaming Wura JSONL data/hau_Latn/hau_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:44:40,604 - INFO - Wrote Wura JSONL → preprocessed_data/hau_Latn/hau_wura_documentLevel.parquet\n",
      "2025-05-14 14:44:40,605 - INFO - Processing language: amh_Ethi\n",
      "2025-05-14 14:44:40,606 - INFO - Processing file: amh_wura.txt\n",
      "2025-05-14 14:44:40,607 - INFO - Processing file: amh_Ethi_ml400.jsonl\n",
      "2025-05-14 14:44:40,607 - INFO - Processing file: amh_wura_documentLevel.jsonl\n",
      "2025-05-14 14:44:55,794 - ERROR - Error streaming Wura JSONL data/amh_Ethi/amh_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:44:55,800 - INFO - Wrote Wura JSONL → preprocessed_data/amh_Ethi/amh_wura_documentLevel.parquet\n",
      "2025-05-14 14:44:55,802 - INFO - Processing file: amh_Ethi_0001_fw2.parquet\n",
      "2025-05-14 14:44:55,803 - INFO - Processing language: xho_Latn\n",
      "2025-05-14 14:44:55,805 - INFO - Processing file: xho_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:44:55,806 - INFO - Processing file: xho_Latn_ml400.jsonl\n",
      "2025-05-14 14:44:55,806 - INFO - Processing file: xho_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:01,038 - ERROR - Error streaming Wura JSONL data/xho_Latn/xho_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:45:01,046 - INFO - Wrote Wura JSONL → preprocessed_data/xho_Latn/xho_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:01,050 - INFO - Processing file: xho_wura.txt\n",
      "2025-05-14 14:45:01,050 - INFO - Processing language: kik_Latn\n",
      "2025-05-14 14:45:01,053 - INFO - Processing file: kik_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:01,054 - INFO - Processing language: aka_Latn\n",
      "2025-05-14 14:45:01,055 - INFO - Processing file: aka_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:01,055 - INFO - Processing language: tso_Latn\n",
      "2025-05-14 14:45:01,056 - INFO - Processing file: tso_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:01,057 - INFO - Processing file: tso_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:01,058 - INFO - Processing language: nya_Latn\n",
      "2025-05-14 14:45:01,060 - INFO - Processing file: nya_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:07,013 - ERROR - Error streaming Wura JSONL data/nya_Latn/nya_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:45:07,020 - INFO - Wrote Wura JSONL → preprocessed_data/nya_Latn/nya_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:07,021 - INFO - Processing file: nya_wura.txt\n",
      "2025-05-14 14:45:07,022 - INFO - Processing file: nya_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:07,023 - INFO - Processing language: tsn_Latn\n",
      "2025-05-14 14:45:07,024 - INFO - Processing file: tsn_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:07,024 - INFO - Processing file: tsn_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:07,025 - INFO - Processing file: tsn_Latn_extra.jsonl\n",
      "2025-05-14 14:45:07,026 - INFO - Processing language: sna_Latn\n",
      "2025-05-14 14:45:07,027 - INFO - Processing file: sna_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:07,028 - INFO - Processing file: sna_wura.txt\n",
      "2025-05-14 14:45:07,028 - INFO - Processing file: sna_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:10,252 - ERROR - Error streaming Wura JSONL data/sna_Latn/sna_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:45:10,257 - INFO - Wrote Wura JSONL → preprocessed_data/sna_Latn/sna_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:10,259 - INFO - Processing file: sna_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:10,260 - INFO - Processing language: nso_Latn\n",
      "2025-05-14 14:45:10,262 - INFO - Processing file: nso_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:10,262 - INFO - Processing language: wol_Latn\n",
      "2025-05-14 14:45:10,263 - INFO - Processing file: wol_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:10,265 - INFO - Processing file: wol_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:10,266 - INFO - Processing language: luo_Latn\n",
      "2025-05-14 14:45:10,267 - INFO - Processing file: luo_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:10,267 - INFO - Processing language: cjk_Latn\n",
      "2025-05-14 14:45:10,269 - INFO - Processing file: cjk_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:10,269 - INFO - Processing language: ibo_Latn\n",
      "2025-05-14 14:45:10,270 - INFO - Processing file: ibo_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:10,271 - INFO - Processing file: ibo_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:10,272 - INFO - Processing file: ibo_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:13,978 - ERROR - Error streaming Wura JSONL data/ibo_Latn/ibo_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:45:13,984 - INFO - Wrote Wura JSONL → preprocessed_data/ibo_Latn/ibo_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:13,985 - INFO - Processing file: ibo_wura.txt\n",
      "2025-05-14 14:45:13,985 - INFO - Processing language: kbp_Latn\n",
      "2025-05-14 14:45:13,987 - INFO - Processing file: kbp_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:13,988 - INFO - Processing file: kbp_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:13,988 - INFO - Processing language: mos_Latn\n",
      "2025-05-14 14:45:13,989 - INFO - Processing file: mos_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:13,990 - INFO - Processing language: bem_Latn\n",
      "2025-05-14 14:45:13,990 - INFO - Processing file: bem_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:13,991 - INFO - Processing language: gaz_Latn\n",
      "2025-05-14 14:45:13,992 - INFO - Processing file: orm_wura.txt\n",
      "2025-05-14 14:45:13,993 - INFO - Processing file: gaz_Latn_ml400.jsonl\n",
      "2025-05-14 14:45:13,993 - INFO - Processing file: orm_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:16,385 - ERROR - Error streaming Wura JSONL data/gaz_Latn/orm_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:45:16,391 - INFO - Wrote Wura JSONL → preprocessed_data/gaz_Latn/orm_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:16,392 - INFO - Processing file: gaz_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:45:16,392 - INFO - Processing language: arz_Arab\n",
      "2025-05-14 14:45:16,394 - INFO - Processing file: arz_Arab_0001_fw2.parquet\n",
      "2025-05-14 14:45:16,394 - INFO - Processing file: arz_wura.txt\n",
      "2025-05-14 14:45:16,395 - INFO - Processing file: arz_wura_documentLevel.jsonl\n",
      "2025-05-14 14:45:26,250 - ERROR - Error streaming Wura JSONL data/arz_Arab/arz_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:45:26,258 - INFO - Wrote Wura JSONL → preprocessed_data/arz_Arab/arz_wura_documentLevel.parquet\n",
      "2025-05-14 14:45:26,258 - INFO - Processing language: eng_Latn\n",
      "2025-05-14 14:45:26,260 - INFO - Processing file: eng_wura_documentLevel.jsonl\n",
      "2025-05-14 14:46:13,212 - ERROR - Error streaming Wura JSONL data/eng_Latn/eng_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:46:13,219 - INFO - Wrote Wura JSONL → preprocessed_data/eng_Latn/eng_wura_documentLevel.parquet\n",
      "2025-05-14 14:46:13,220 - INFO - Processing file: eng_Latn_0004_fw2.parquet\n",
      "2025-05-14 14:46:13,221 - INFO - Processing file: eng_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:13,222 - INFO - Processing file: eng_Latn_0008_fw2.parquet\n",
      "2025-05-14 14:46:13,222 - INFO - Processing file: eng_Latn_0002_fw2.parquet\n",
      "2025-05-14 14:46:13,223 - INFO - Processing file: eng_Latn_0007_fw2.parquet\n",
      "2025-05-14 14:46:13,224 - INFO - Processing file: eng_Latn_0009_fw2.parquet\n",
      "2025-05-14 14:46:13,224 - INFO - Processing file: eng_Latn_0005_fw2.parquet\n",
      "2025-05-14 14:46:13,226 - INFO - Processing file: eng_Latn_0010_fw2.parquet\n",
      "2025-05-14 14:46:13,226 - INFO - Processing file: eng_Latn_0006_fw2.parquet\n",
      "2025-05-14 14:46:13,228 - INFO - Processing file: eng_wura.txt\n",
      "2025-05-14 14:46:13,228 - INFO - Processing file: eng_Latn_0003_fw2.parquet\n",
      "2025-05-14 14:46:13,229 - INFO - Processing language: dyu_Latn\n",
      "2025-05-14 14:46:13,230 - INFO - Processing file: dyu_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:13,231 - INFO - Processing file: dyu_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:13,231 - INFO - Processing language: tir_Ethi\n",
      "2025-05-14 14:46:13,233 - INFO - Processing file: tir_wura_documentLevel.jsonl\n",
      "2025-05-14 14:46:22,386 - ERROR - Error streaming Wura JSONL data/tir_Ethi/tir_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:46:22,390 - INFO - Wrote Wura JSONL → preprocessed_data/tir_Ethi/tir_wura_documentLevel.parquet\n",
      "2025-05-14 14:46:22,391 - INFO - Processing file: tir_Ethi_0001_fw2.parquet\n",
      "2025-05-14 14:46:22,392 - INFO - Processing file: tir_wura.txt\n",
      "2025-05-14 14:46:22,392 - INFO - Processing file: tir_Ethi_ml400.jsonl\n",
      "2025-05-14 14:46:22,393 - INFO - Processing language: run_Latn\n",
      "2025-05-14 14:46:22,394 - INFO - Processing file: run_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:22,394 - INFO - Processing file: run_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:22,395 - INFO - Processing language: som_Latn\n",
      "2025-05-14 14:46:22,396 - INFO - Processing file: som_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:22,396 - INFO - Processing file: som_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:22,397 - INFO - Processing file: som_wura_documentLevel.jsonl\n",
      "2025-05-14 14:46:48,066 - ERROR - Error streaming Wura JSONL data/som_Latn/som_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:46:48,072 - INFO - Wrote Wura JSONL → preprocessed_data/som_Latn/som_wura_documentLevel.parquet\n",
      "2025-05-14 14:46:48,073 - INFO - Processing file: som_wura.txt\n",
      "2025-05-14 14:46:48,074 - INFO - Processing language: arb_Arab\n",
      "2025-05-14 14:46:48,075 - INFO - Processing file: arb_Arab_0009_fw2.parquet\n",
      "2025-05-14 14:46:48,076 - INFO - Processing file: arb_Arab_0005_fw2.parquet\n",
      "2025-05-14 14:46:48,077 - INFO - Processing file: arb_Arab_0010_fw2.parquet\n",
      "2025-05-14 14:46:48,077 - INFO - Processing file: arb_Arab_0006_fw2.parquet\n",
      "2025-05-14 14:46:48,079 - INFO - Processing file: arb_Arab_0003_fw2.parquet\n",
      "2025-05-14 14:46:48,079 - INFO - Processing file: arb_Arab_0004_fw2.parquet\n",
      "2025-05-14 14:46:48,081 - INFO - Processing file: arb_Arab_0001_fw2.parquet\n",
      "2025-05-14 14:46:48,081 - INFO - Processing file: arb_Arab_0008_fw2.parquet\n",
      "2025-05-14 14:46:48,082 - INFO - Processing file: arb_Arab_0002_fw2.parquet\n",
      "2025-05-14 14:46:48,082 - INFO - Processing file: arb_Arab_0007_fw2.parquet\n",
      "2025-05-14 14:46:48,082 - INFO - Processing language: kab_Latn\n",
      "2025-05-14 14:46:48,084 - INFO - Processing file: kab_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:48,085 - INFO - Processing language: yor_Latn\n",
      "2025-05-14 14:46:48,086 - INFO - Processing file: yor_wura_documentLevel.jsonl\n",
      "2025-05-14 14:46:56,539 - ERROR - Error streaming Wura JSONL data/yor_Latn/yor_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:46:56,543 - INFO - Wrote Wura JSONL → preprocessed_data/yor_Latn/yor_wura_documentLevel.parquet\n",
      "2025-05-14 14:46:56,544 - INFO - Processing file: yor_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:56,544 - INFO - Processing file: yor_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,545 - INFO - Processing file: yor_wura.txt\n",
      "2025-05-14 14:46:56,545 - INFO - Processing language: taq_Tfng\n",
      "2025-05-14 14:46:56,546 - INFO - Processing file: taq_Tfng_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,546 - INFO - Processing language: fuv_Latn\n",
      "2025-05-14 14:46:56,546 - INFO - Processing file: fuv_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,547 - INFO - Processing file: fuv_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:56,547 - INFO - Processing language: twi_Latn\n",
      "2025-05-14 14:46:56,548 - INFO - Processing file: twi_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,548 - INFO - Processing language: lin_Latn\n",
      "2025-05-14 14:46:56,549 - INFO - Processing file: lin_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,549 - INFO - Processing file: lin_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:56,550 - INFO - Processing language: knc_Arab\n",
      "2025-05-14 14:46:56,550 - INFO - Processing file: knc_Arab_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,551 - INFO - Processing language: sot_Latn\n",
      "2025-05-14 14:46:56,551 - INFO - Processing file: sot_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,551 - INFO - Processing file: sot_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:56,552 - INFO - Processing language: bam_Latn\n",
      "2025-05-14 14:46:56,553 - INFO - Processing file: bam_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,553 - INFO - Processing file: bam_Latn_ml400.jsonl\n",
      "2025-05-14 14:46:56,553 - INFO - Processing language: por_Latn\n",
      "2025-05-14 14:46:56,554 - INFO - Processing file: por_Latn_0004_fw2.parquet\n",
      "2025-05-14 14:46:56,555 - INFO - Processing file: por_Latn_0011_fw2.parquet\n",
      "2025-05-14 14:46:56,555 - INFO - Processing file: por_Latn_0014_fw2.parquet\n",
      "2025-05-14 14:46:56,556 - INFO - Processing file: por_Latn_0008_fw2.parquet\n",
      "2025-05-14 14:46:56,556 - INFO - Processing file: por_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:46:56,557 - INFO - Processing file: por_Latn_0002_fw2.parquet\n",
      "2025-05-14 14:46:56,557 - INFO - Processing file: por_wura_documentLevel.jsonl\n",
      "2025-05-14 14:47:25,809 - ERROR - Error streaming Wura JSONL data/por_Latn/por_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:47:25,814 - INFO - Wrote Wura JSONL → preprocessed_data/por_Latn/por_wura_documentLevel.parquet\n",
      "2025-05-14 14:47:25,816 - INFO - Processing file: por_Latn_0012_fw2.parquet\n",
      "2025-05-14 14:47:25,817 - INFO - Processing file: por_Latn_0007_fw2.parquet\n",
      "2025-05-14 14:47:25,818 - INFO - Processing file: por_wura.txt\n",
      "2025-05-14 14:47:25,819 - INFO - Processing file: por_Latn_0009_fw2.parquet\n",
      "2025-05-14 14:47:25,820 - INFO - Processing file: por_Latn_0010_fw2.parquet\n",
      "2025-05-14 14:47:25,821 - INFO - Processing file: por_Latn_0005_fw2.parquet\n",
      "2025-05-14 14:47:25,822 - INFO - Processing file: por_Latn_0006_fw2.parquet\n",
      "2025-05-14 14:47:25,823 - INFO - Processing file: por_Latn_0013_fw2.parquet\n",
      "2025-05-14 14:47:25,824 - INFO - Processing file: por_Latn_0003_fw2.parquet\n",
      "2025-05-14 14:47:25,825 - INFO - Processing language: swh_Latn\n",
      "2025-05-14 14:47:25,827 - INFO - Processing file: swh_Latn_ml400.jsonl\n",
      "2025-05-14 14:47:25,828 - INFO - Processing file: swh_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:47:25,829 - INFO - Processing file: swa_wura.txt\n",
      "2025-05-14 14:47:25,830 - INFO - Processing file: swa_wura_documentLevel.jsonl\n",
      "2025-05-14 14:48:07,242 - ERROR - Error streaming Wura JSONL data/swh_Latn/swa_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:48:07,251 - INFO - Wrote Wura JSONL → preprocessed_data/swh_Latn/swa_wura_documentLevel.parquet\n",
      "2025-05-14 14:48:07,251 - INFO - Processing language: kam_Latn\n",
      "2025-05-14 14:48:07,254 - INFO - Processing file: kam_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,254 - INFO - Processing language: kmb_Latn\n",
      "2025-05-14 14:48:07,256 - INFO - Processing file: kmb_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,257 - INFO - Processing file: kmb_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,257 - INFO - Processing language: aeb_Arab\n",
      "2025-05-14 14:48:07,259 - INFO - Processing file: aeb_Arab_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,259 - INFO - Processing language: plt_Latn\n",
      "2025-05-14 14:48:07,260 - INFO - Processing file: plt_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,261 - INFO - Processing language: ewe_Latn\n",
      "2025-05-14 14:48:07,262 - INFO - Processing file: ewe_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,263 - INFO - Processing file: ewe_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,263 - INFO - Processing language: kon_Latn\n",
      "2025-05-14 14:48:07,265 - INFO - Processing file: kon_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,265 - INFO - Processing language: kea_Latn\n",
      "2025-05-14 14:48:07,266 - INFO - Processing file: kea_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,267 - INFO - Processing language: dik_Latn\n",
      "2025-05-14 14:48:07,267 - INFO - Processing file: dik_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,268 - INFO - Processing file: dik_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,268 - INFO - Processing language: lug_Latn\n",
      "2025-05-14 14:48:07,269 - INFO - Processing file: lug_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,269 - INFO - Processing file: lug_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,270 - INFO - Processing language: ary_Arab\n",
      "2025-05-14 14:48:07,270 - INFO - Processing file: ary_Arab_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,271 - INFO - Processing file: ary_Arab_0002_fw2.parquet\n",
      "2025-05-14 14:48:07,271 - INFO - Processing language: kin_Latn\n",
      "2025-05-14 14:48:07,272 - INFO - Processing file: kin_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:07,272 - INFO - Processing file: kin_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:07,273 - INFO - Processing file: kin_wura_documentLevel.jsonl\n",
      "2025-05-14 14:48:10,705 - ERROR - Error streaming Wura JSONL data/kin_Latn/kin_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:48:10,713 - INFO - Wrote Wura JSONL → preprocessed_data/kin_Latn/kin_wura_documentLevel.parquet\n",
      "2025-05-14 14:48:10,714 - INFO - Processing file: kin_wura.txt\n",
      "2025-05-14 14:48:10,715 - INFO - Processing language: nus_Latn\n",
      "2025-05-14 14:48:10,716 - INFO - Processing file: nus_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:10,717 - INFO - Processing language: sag_Latn\n",
      "2025-05-14 14:48:10,718 - INFO - Processing file: sag_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:10,719 - INFO - Processing file: sag_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:10,719 - INFO - Processing language: fra_Latn\n",
      "2025-05-14 14:48:10,720 - INFO - Processing file: fra_Latn_0003_fw2.parquet\n",
      "2025-05-14 14:48:10,721 - INFO - Processing file: fra_Latn_0006_fw2.parquet\n",
      "2025-05-14 14:48:10,722 - INFO - Processing file: fra_Latn_0013_fw2.parquet\n",
      "2025-05-14 14:48:10,722 - INFO - Processing file: fra_Latn_0010_fw2.parquet\n",
      "2025-05-14 14:48:10,723 - INFO - Processing file: fra_Latn_0005_fw2.parquet\n",
      "2025-05-14 14:48:10,724 - INFO - Processing file: fra_wura_documentLevel.jsonl\n",
      "2025-05-14 14:48:54,610 - ERROR - Error streaming Wura JSONL data/fra_Latn/fra_wura_documentLevel.jsonl: [Errno 60] Operation timed out\n",
      "2025-05-14 14:48:54,617 - INFO - Wrote Wura JSONL → preprocessed_data/fra_Latn/fra_wura_documentLevel.parquet\n",
      "2025-05-14 14:48:54,618 - INFO - Processing file: fra_Latn_0009_fw2.parquet\n",
      "2025-05-14 14:48:54,619 - INFO - Processing file: fra_Latn_0012_fw2.parquet\n",
      "2025-05-14 14:48:54,621 - INFO - Processing file: fra_Latn_0007_fw2.parquet\n",
      "2025-05-14 14:48:54,621 - INFO - Processing file: fra_Latn_0002_fw2.parquet\n",
      "2025-05-14 14:48:54,622 - INFO - Processing file: fra_Latn_0008_fw2.parquet\n",
      "2025-05-14 14:48:54,622 - INFO - Processing file: fra_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,624 - INFO - Processing file: fra_wura.txt\n",
      "2025-05-14 14:48:54,624 - INFO - Processing file: fra_Latn_0004_fw2.parquet\n",
      "2025-05-14 14:48:54,625 - INFO - Processing file: fra_Latn_0011_fw2.parquet\n",
      "2025-05-14 14:48:54,626 - INFO - Processing language: umb_Latn\n",
      "2025-05-14 14:48:54,627 - INFO - Processing file: umb_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,627 - INFO - Processing language: tzm_Tfng\n",
      "2025-05-14 14:48:54,629 - INFO - Processing file: tzm_Tfng_ml400.jsonl\n",
      "2025-05-14 14:48:54,629 - INFO - Processing file: tzm_Tfng_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,630 - INFO - Processing language: nqo_Nkoo\n",
      "2025-05-14 14:48:54,632 - INFO - Processing file: nqo_Nkoo_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,633 - INFO - Processing language: knc_Latn\n",
      "2025-05-14 14:48:54,634 - INFO - Processing file: knc_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,635 - INFO - Processing language: lua_Latn\n",
      "2025-05-14 14:48:54,637 - INFO - Processing file: lua_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,638 - INFO - Processing language: afr_Latn\n",
      "2025-05-14 14:48:54,640 - INFO - Processing file: afr_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:48:54,641 - INFO - Processing file: afr_Latn_ml400.jsonl\n",
      "2025-05-14 14:48:54,642 - INFO - Processing file: afr_wura.txt\n",
      "2025-05-14 14:48:54,643 - INFO - Processing file: afr_wura_documentLevel.jsonl\n",
      "2025-05-14 14:51:35,402 - ERROR - Error streaming Wura JSONL data/afr_Latn/afr_wura_documentLevel.jsonl: 'DatasetTransformer' object has no attribute 'chunk_size'\n",
      "2025-05-14 14:51:35,405 - INFO - Wrote Wura JSONL → preprocessed_data/afr_Latn/afr_wura_documentLevel.parquet\n",
      "2025-05-14 14:51:35,405 - INFO - Processing language: tum_Latn\n",
      "2025-05-14 14:51:35,406 - INFO - Processing file: tum_Latn_0001_fw2.parquet\n",
      "2025-05-14 14:51:35,406 - INFO - Processing language: zul_Latn\n",
      "2025-05-14 14:51:35,407 - INFO - Processing file: zul_wura_documentLevel.jsonl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 390\u001b[39m\n\u001b[32m    386\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mDataset transformation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 385\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    382\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mpreprocessed_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    384\u001b[39m transformer = DatasetTransformer(root_dir=input_dir, output_dir=output_dir)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mDataset transformation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mDatasetTransformer.process_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m lang_dir.glob(\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_path.is_file():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mDatasetTransformer.process_file\u001b[39m\u001b[34m(self, file_path, output_dir, lang_code)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# JSONL-based Wura\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.jsonl\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mwura\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m filename:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_wura_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Determine file type\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mDatasetTransformer.process_wura_jsonl\u001b[39m\u001b[34m(self, file_path, lang_code, output_path)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhyperparam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatasetTransformer:\n",
    "    def __init__(self, root_dir=\"data\", output_dir=\"preprocessed_data\"):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "    def scan_directory(self):\n",
    "        \"\"\"Scan the data directory for language subdirectories and files.\"\"\"\n",
    "        lang_dirs = [d for d in self.root_dir.iterdir() if d.is_dir()]\n",
    "        logger.info(f\"Found {len(lang_dirs)} language directories\")\n",
    "        return lang_dirs\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all language directories and their files.\"\"\"\n",
    "        lang_dirs = self.scan_directory()\n",
    "        \n",
    "        for lang_dir in lang_dirs:\n",
    "            lang_code = lang_dir.name\n",
    "            logger.info(f\"Processing language: {lang_code}\")\n",
    "            \n",
    "            # Create output directory for this language\n",
    "            lang_output_dir = self.output_dir / lang_code\n",
    "            lang_output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Process each file in the language directory\n",
    "            for file_path in lang_dir.glob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    self.process_file(file_path, lang_output_dir, lang_code)\n",
    "    \n",
    "    def process_file(self, file_path, output_dir, lang_code):\n",
    "        \"\"\"Process a single file based on its type and name pattern.\"\"\"\n",
    "        filename = file_path.name\n",
    "        logger.info(f\"Processing file: {filename}\")\n",
    "        output_path = output_dir / f\"{file_path.stem}.parquet\"\n",
    "\n",
    "        # ←— Skip if we already wrote this file\n",
    "        if output_path.exists():\n",
    "            logger.info(f\"Skipping {filename}: output already at {output_path}\")\n",
    "            return\n",
    "        \n",
    "        # Determine file type\n",
    "        if filename.endswith(\".jsonl\") and \"wura\" in filename: # This will be the document level of wura\n",
    "            self.process_wura_jsonl(file_path, lang_code, output_path)\n",
    "            return\n",
    "        if filename.endswith(\".parquet\") and \"fw2\" in filename:\n",
    "            self.process_fw2(file_path, lang_code, output_path)\n",
    "            logger.info(f\"Chunk-wrote FW2 to {output_path}\")\n",
    "            return\n",
    "        elif filename.endswith(\".txt\") and \"wura\" in filename: # This will be the passage level of wura\n",
    "            # pass output_path so it can write itself\n",
    "            self.process_wura(file_path, lang_code, output_path)\n",
    "            logger.info(f\"Chunk-wrote Wura → {output_path}\")\n",
    "            return\n",
    "        elif filename.endswith(\".jsonl\") and \"ml400\" in filename:\n",
    "            self.process_madlad400(file_path, lang_code, output_path)\n",
    "            logger.info(f\"Chunk-wrote MADLAD400 → {output_path}\")\n",
    "            return\n",
    "        elif filename.endswith(\".jsonl\") and \"extra\" in filename:\n",
    "            df = self.process_extra_data(file_path, lang_code)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown file format for {filename}, skipping\")\n",
    "            return\n",
    "        \n",
    "        # Save the transformed dataframe\n",
    "        output_filename = f\"{file_path.stem}.parquet\"\n",
    "        output_path = output_dir / output_filename\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        logger.info(f\"Saved transformed data to {output_path}\")\n",
    "\n",
    "    def process_wura_jsonl(self, file_path: Path, lang_code: str, output_path: Path):\n",
    "        \"\"\"Stream Wura JSONL in chunks and write to Parquet.\"\"\"\n",
    "        writer = None\n",
    "        batch, n = [], 0\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    entry = json.loads(line)\n",
    "                    batch.append({\n",
    "                        'text': entry.get('text', ''),\n",
    "                        'hyperparam': {\n",
    "                            'id':       f\"wura_{lang_code}_{i}\",\n",
    "                            'language': lang_code,\n",
    "                            'dataset':  'wura'\n",
    "                        }\n",
    "                    })\n",
    "                    n += 1\n",
    "                    if n >= self.chunk_size:\n",
    "                        tbl = pa.Table.from_pandas(\n",
    "                            pd.DataFrame(batch)[['text','hyperparam']],\n",
    "                            preserve_index=False\n",
    "                        )\n",
    "                        if writer is None:\n",
    "                            writer = pq.ParquetWriter(str(output_path), schema=tbl.schema)\n",
    "                        writer.write_table(tbl)\n",
    "                        batch, n = [], 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error streaming Wura JSONL {file_path}: {e}\")\n",
    "\n",
    "        # flush remainder\n",
    "        if batch:\n",
    "            tbl = pa.Table.from_pandas(\n",
    "                pd.DataFrame(batch)[['text','hyperparam']],\n",
    "                preserve_index=False\n",
    "            )\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(output_path), schema=tbl.schema)\n",
    "            writer.write_table(tbl)\n",
    "\n",
    "        # if nothing written, stub out an empty file\n",
    "        if writer:\n",
    "            writer.close()\n",
    "        else:\n",
    "            empty = pa.Table.from_pandas(\n",
    "                pd.DataFrame(columns=['text','hyperparam']),\n",
    "                preserve_index=False\n",
    "            )\n",
    "            pq.write_table(empty, str(output_path))\n",
    "        logger.info(f\"Wrote Wura JSONL → {output_path}\")\n",
    "    \n",
    "    def process_fw2(self, file_path, lang_code, output_path):\n",
    "        \"\"\"\n",
    "        Process one large FW2 parquet by row-group, build hyperparam, and stream directly\n",
    "        to output_path (a pathlib.Path to the final .parquet).\n",
    "        \"\"\"\n",
    "        # List every field you *want*, but we'll only read the ones that actually exist\n",
    "        wanted = [\n",
    "            'text','id','dump','url','date','file_path',\n",
    "            'language','language_score','language_script',\n",
    "            'minhash_cluster_size','top_langs'\n",
    "        ]\n",
    "\n",
    "        # Open the source Parquet\n",
    "        pqf = pq.ParquetFile(str(file_path))\n",
    "        writer = None\n",
    "\n",
    "        for rg in range(pqf.num_row_groups):\n",
    "            # 1) Figure out which of our \"wanted\" fields are actually in this file\n",
    "            existing = set(pqf.schema.names)\n",
    "            to_read  = [c for c in wanted if c in existing]\n",
    "\n",
    "            # always require \"text\" so we can output something\n",
    "            if 'text' not in to_read:\n",
    "                logger.warning(f\"No 'text' column in row-group {rg}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # 2) Read just those columns\n",
    "            table = pqf.read_row_group(rg, columns=to_read)\n",
    "            df    = table.to_pandas()\n",
    "\n",
    "            # 3) For any field we *wanted* but wasn’t present, add a default\n",
    "            for c in wanted:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = None\n",
    "\n",
    "            # 4) Build hyperparam *safely* using row.get() with defaults\n",
    "            def make_hp(row):\n",
    "                return {\n",
    "                    'id':                    row.get('id', '')                   or '',\n",
    "                    'dump':                  row.get('dump', '')                 or '',\n",
    "                    'url':                   row.get('url', '')                  or '',\n",
    "                    'date':                  str(row.get('date', ''))            or '',\n",
    "                    'file_path':             row.get('file_path', '')            or '',\n",
    "                    'language':              lang_code,\n",
    "                    'language_score':        row.get('language_score', 0.0)      or 0.0,\n",
    "                    'language_script':       row.get('language_script', '')      or '',\n",
    "                    'minhash_cluster_size':  row.get('minhash_cluster_size', 0) or 0,\n",
    "                    'top_langs':             row.get('top_langs', [])            or [],\n",
    "                    'dataset':               'fw2'\n",
    "                }\n",
    "\n",
    "            df['hyperparam'] = df.apply(make_hp, axis=1)\n",
    "\n",
    "            # 5) Stream it straight out to Parquet without concatenating in RAM\n",
    "            out_table = pa.Table.from_pandas(\n",
    "                df[['text','hyperparam']],\n",
    "                preserve_index=False\n",
    "            )\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(output_path), schema=out_table.schema)\n",
    "            writer.write_table(out_table)\n",
    "\n",
    "            # free up memory\n",
    "            del df, table, out_table\n",
    "\n",
    "        # 6) Clean up\n",
    "        if writer:\n",
    "            writer.close()\n",
    "        else:\n",
    "            # no data at all? write an empty stub\n",
    "            empty = pa.Table.from_pandas(\n",
    "                pd.DataFrame(columns=['text','hyperparam']),\n",
    "                preserve_index=False\n",
    "            )\n",
    "            pq.write_table(empty, str(output_path))\n",
    "\n",
    "    \n",
    "    def process_wura(self, file_path, lang_code, output_path, chunk_size=50_000):\n",
    "        \"\"\"Process Wura `.txt` by streaming in chunks and writing to Parquet.\"\"\"\n",
    "        writer = None\n",
    "        rows = []\n",
    "        count = 0\n",
    "\n",
    "        # 1) Stream the file line by line\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.rstrip('\\n')\n",
    "                # parse id/text\n",
    "                if '\\t' in line:\n",
    "                    doc_id, text = line.split('\\t', 1)\n",
    "                else:\n",
    "                    doc_id = f\"wura_{lang_code}_{i}\"\n",
    "                    text   = line\n",
    "\n",
    "                rows.append({\n",
    "                    'text': text,\n",
    "                    'hyperparam': {\n",
    "                        'id':       doc_id,\n",
    "                        'language': lang_code,\n",
    "                        'dataset':  'wura'\n",
    "                    }\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "                # 2) Once we hit chunk_size, flush to Parquet\n",
    "                if count >= chunk_size:\n",
    "                    df = pd.DataFrame(rows)\n",
    "                    table = pa.Table.from_pandas(df[['text','hyperparam']],\n",
    "                                                preserve_index=False)\n",
    "                    if writer is None:\n",
    "                        writer = pq.ParquetWriter(str(output_path),\n",
    "                                                schema=table.schema)\n",
    "                    writer.write_table(table)\n",
    "\n",
    "                    # reset\n",
    "                    rows = []\n",
    "                    count = 0\n",
    "\n",
    "        # 3) Flush any remaining rows\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            table = pa.Table.from_pandas(df[['text','hyperparam']],\n",
    "                                        preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(output_path),\n",
    "                                        schema=table.schema)\n",
    "            writer.write_table(table)\n",
    "\n",
    "        # 4) If we never saw any data, write an empty stub\n",
    "        if writer:\n",
    "            writer.close()\n",
    "        else:\n",
    "            empty = pa.Table.from_pandas(\n",
    "                pd.DataFrame(columns=['text','hyperparam']),\n",
    "                preserve_index=False\n",
    "            )\n",
    "            pq.write_table(empty, str(output_path))\n",
    "\n",
    "        logger.info(f\"Wrote Wura data for {lang_code}: {output_path}\")\n",
    "        \n",
    "    def process_madlad400(self, file_path, lang_code, output_path, chunk_size=50_000):\n",
    "        \"\"\"Process MADLAD400 JSONL by streaming in chunks and writing to Parquet.\"\"\"\n",
    "        writer = None\n",
    "        batch = []\n",
    "        n = 0\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    try:\n",
    "                        entry = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        logger.warning(f\"Invalid JSON on line {i} in {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    text = entry.get('text', '')\n",
    "                    batch.append({\n",
    "                        'text': text,\n",
    "                        'hyperparam': {\n",
    "                            'id':       f\"madlad400_{lang_code}_{i}\",\n",
    "                            'language': lang_code,\n",
    "                            'dataset':  'madlad400'\n",
    "                        }\n",
    "                    })\n",
    "                    n += 1\n",
    "\n",
    "                    # flush chunk\n",
    "                    if n >= chunk_size:\n",
    "                        tbl = pa.Table.from_pandas(\n",
    "                            pd.DataFrame(batch)[['text','hyperparam']],\n",
    "                            preserve_index=False\n",
    "                        )\n",
    "                        if writer is None:\n",
    "                            writer = pq.ParquetWriter(str(output_path), schema=tbl.schema)\n",
    "                        writer.write_table(tbl)\n",
    "                        batch, n = [], 0\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error streaming MADLAD400 {file_path}: {e}\")\n",
    "\n",
    "        # flush remainder\n",
    "        if batch:\n",
    "            tbl = pa.Table.from_pandas(\n",
    "                pd.DataFrame(batch)[['text','hyperparam']],\n",
    "                preserve_index=False\n",
    "            )\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(output_path), schema=tbl.schema)\n",
    "            writer.write_table(tbl)\n",
    "\n",
    "        # if nothing written, stub out an empty file\n",
    "        if writer:\n",
    "            writer.close()\n",
    "        else:\n",
    "            empty = pa.Table.from_pandas(\n",
    "                pd.DataFrame(columns=['text','hyperparam']),\n",
    "                preserve_index=False\n",
    "            )\n",
    "            pq.write_table(empty, str(output_path))\n",
    "\n",
    "        logger.info(f\"Wrote MADLAD400 for {lang_code}: {output_path}\")\n",
    "    \n",
    "    def process_extra_data(self, file_path, lang_code):\n",
    "        \"\"\"Process extra data JSONL files.\"\"\"\n",
    "        try:\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    try:\n",
    "                        # Parse the JSON line\n",
    "                        entry = json.loads(line)\n",
    "                        text = entry.get('text', '')\n",
    "                        source = entry.get('source', '')\n",
    "                        source_category = entry.get('source-category', '')\n",
    "                        \n",
    "                        data.append({\n",
    "                            'text': text,\n",
    "                            'hyperparam': {\n",
    "                                'id': f\"extra_{lang_code}_{i}\",\n",
    "                                'language': lang_code,\n",
    "                                'source': source,\n",
    "                                'source_category': source_category,\n",
    "                                'dataset': 'extra'\n",
    "                            }\n",
    "                        })\n",
    "                    except json.JSONDecodeError:\n",
    "                        logger.warning(f\"Invalid JSON on line {i} in file {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error processing line {i} in extra data file: {e}\")\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing extra data file {file_path}: {e}\")\n",
    "            return pd.DataFrame(columns=['text', 'hyperparam'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the dataset transformer.\"\"\"\n",
    "    # Configure these paths as needed\n",
    "    input_dir = \"data\"\n",
    "    output_dir = \"preprocessed_data\"\n",
    "    \n",
    "    transformer = DatasetTransformer(root_dir=input_dir, output_dir=output_dir)\n",
    "    transformer.process_all()\n",
    "    logger.info(\"Dataset transformation complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51109ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: {'text': Value(dtype='string', id=None), 'hyperparam': {'dataset': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None)}, 'dataset_origin': Value(dtype='string', id=None)}\n",
      "First example: {'text': 'طائر المغرد البيروفى طائر المغرد البيروفى\\n\\nطائر المغرد البيروفى ( الاسم العلمى: Hypocnemis peruviana ) هوا نوع من الطيور بيتبع هيپوكنيميس.', 'hyperparam': {'dataset': 'wura', 'id': 'wura_arz_Arab_0', 'language': 'arz_Arab'}, 'dataset_origin': 'wuraDocumentLevel'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"preprocessed_data/arz_Arab/arz_wura_documentLevel.parquet\"},\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"Available columns:\", dataset.features)\n",
    "print(\"First example:\", next(iter(dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1a7cd",
   "metadata": {},
   "source": [
    "# Create My Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "895ed71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages: ['aeb_Arab', 'afr_Latn', 'aka_Latn', 'amh_Ethi', 'arb_Arab', 'ary_Arab', 'arz_Arab', 'bam_Latn', 'bem_Latn', 'cjk_Latn', 'dik_Latn', 'dyu_Latn', 'eng_Latn', 'ewe_Latn', 'fon_Latn', 'fra_Latn', 'fuv_Latn', 'gaz_Latn', 'hau_Latn', 'ibo_Latn', 'kab_Latn', 'kam_Latn', 'kbp_Latn', 'kea_Latn', 'kik_Latn', 'kin_Latn', 'kmb_Latn', 'knc_Arab', 'knc_Latn', 'kon_Latn', 'lin_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'mos_Latn', 'nqo_Nkoo', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'plt_Latn', 'por_Latn', 'run_Latn', 'sag_Latn', 'sna_Latn', 'som_Latn', 'sot_Latn', 'ssw_Latn', 'swh_Latn', 'taq_Latn', 'taq_Tfng', 'tir_Ethi', 'tsn_Latn', 'tso_Latn', 'tum_Latn', 'twi_Latn', 'tzm_Tfng', 'umb_Latn', 'wol_Latn', 'xho_Latn', 'yor_Latn', 'zul_Latn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fineweb2 split: 262884 examples [00:00, 309567.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    fineweb2: Dataset({\n",
      "        features: ['text', 'hyperparam', 'dataset_origin'],\n",
      "        num_rows: 262884\n",
      "    })\n",
      "})\n",
      "{'text': 'هم كثير مشاء الله\\nمن السابقون الشيخ عبد الباسط رحمة الله و الشيخ البناء عليه رحمة الله و السيخ الحصري ادخلة الله فسيح جناته\\nاما من الاحياء اطال الله اعمارهم الشيخ مشاري العفاسي و الشيخ شيخ ابو بكر الشاطري و انا اعشق صوت الشيخ عبد الرشيد صوفي\\nعليهم جميعاً رجمة الله وبركاته وجمعنا الله واياهم في الجنان مع رسول الله و الصحابة و الشهداء\\nامين امين امين', 'hyperparam': {'dataset': 'fw2', 'date': '2013-06-20T06:03:56Z', 'dump': 'CC-MAIN-2013-20', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368710366143/warc/CC-MAIN-20130516131926-00095-ip-10-60-113-184.ec2.internal.warc.gz', 'id': '<urn:uuid:ab7fab11-f6e8-4919-a6e8-8b62a2a32938>', 'language': 'aeb_Arab', 'language_score': 0.4684912860393524, 'language_script': 'Arab', 'minhash_cluster_size': 2, 'top_langs': '{\"aeb_Arab_score\": 0.4684912860393524, \"ars_Arab_score\": 0.17445436120033264, \"ajp_Arab_score\": 0.16118812561035156, \"ary_Arab_score\": 0.10148709267377853, \"arz_Arab_score\": 0.06104366108775139, \"apc_Arab_score\": 0.020420128479599953, \"arb_Arab_score\": 0.011830095201730728}', 'url': 'http://ejabat.google.com/ejabat/thread?tid=14720117374f7663'}, 'dataset_origin': 'fw2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "configs = get_dataset_config_names(\"Tiany1/multilingualPretrainDataset\")\n",
    "\n",
    "print(\"Available languages:\", configs)\n",
    "\n",
    "\n",
    "ds_aeb = load_dataset(\"Tiany1/multilingualPretrainDataset\", \"aeb_Arab\")\n",
    "\n",
    "print(ds_aeb)\n",
    "\n",
    "print(ds_aeb[\"fineweb2\"][0])        # first record from the fineweb2 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246d2d9",
   "metadata": {},
   "source": [
    "# Command for Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34988eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages: ['aeb_Arab', 'afr_Latn', 'aka_Latn', 'amh_Ethi', 'arb_Arab', 'ary_Arab', 'arz_Arab', 'bam_Latn', 'bem_Latn', 'cjk_Latn', 'dik_Latn', 'dyu_Latn', 'eng_Latn', 'ewe_Latn', 'fon_Latn', 'fra_Latn', 'fuv_Latn', 'gaz_Latn', 'hau_Latn', 'ibo_Latn', 'kab_Latn', 'kam_Latn', 'kbp_Latn', 'kea_Latn', 'kik_Latn', 'kin_Latn', 'kmb_Latn', 'knc_Arab', 'knc_Latn', 'kon_Latn', 'lin_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'mos_Latn', 'nqo_Nkoo', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'plt_Latn', 'por_Latn', 'run_Latn', 'sag_Latn', 'sna_Latn', 'som_Latn', 'sot_Latn', 'ssw_Latn', 'swh_Latn', 'taq_Latn', 'taq_Tfng', 'tir_Ethi', 'tsn_Latn', 'tso_Latn', 'tum_Latn', 'twi_Latn', 'tzm_Tfng', 'umb_Latn', 'wol_Latn', 'xho_Latn', 'yor_Latn', 'zul_Latn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fineweb2 split: 280355 examples [00:02, 114262.66 examples/s]\n",
      "Generating madlad400 split: 106301 examples [00:01, 56247.37 examples/s]\n",
      "Generating wura_passageLevel split: 291026 examples [00:00, 299292.19 examples/s]\n",
      "Generating wura_documentLevel split: 135863 examples [00:01, 85321.96 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    fineweb2: Dataset({\n",
      "        features: ['text', 'hyperparam', 'dataset_origin'],\n",
      "        num_rows: 280355\n",
      "    })\n",
      "    madlad400: Dataset({\n",
      "        features: ['text', 'hyperparam', 'dataset_origin'],\n",
      "        num_rows: 106301\n",
      "    })\n",
      "    wura_passageLevel: Dataset({\n",
      "        features: ['text', 'hyperparam', 'dataset_origin'],\n",
      "        num_rows: 291026\n",
      "    })\n",
      "    wura_documentLevel: Dataset({\n",
      "        features: ['text', 'hyperparam', 'dataset_origin'],\n",
      "        num_rows: 135863\n",
      "    })\n",
      "})\n",
      "{'text': 'ህጻን ልጃቸውን ለ7 ዓመታት የቆለፉባት ጀርመናውያን ቤተሰቦች ምርመራ ተከፈተባቸው በጀርመን የስምንት ዓመት ህፃን ልጅ ላይ ለአመታት ከቤት እንዳትወጣ የቆለፉት እናት እና አያቶች ምርመራ ተከፈተባቸው። የጀርመን አቃቤ ህግ ህጻኗ ለሰባት ዓመታት ያህል በቤት ውስጥ ተዘግቶባት እንደነበር አስታውቋል። በመጨረሻም በያዝነው ዓመት መስከረም መገባደጃ ላይ ነፃ እንደወጣችና ለማደጎም ተሰጥታለች ተብሏል። በህጻናት ደህንነት ላይ የሚሰሩ ባለስልጣናት ቀላል የሚባሉ እንደ ደረጃም መውጣትም ሆነ የዕለት ተዕለት ተግባራትን ለማከናወን እንደምትቸገር ተናግረዋል። የጀርመን ሚዲያዎች ውጭ ወጥታ እንደማታውቅና ጫካም ሆነ ሜዳ አይታም ሆነ ምንነቱንም እንደማታውቅ ዘግበዋል። እናቷ ኑሯችንን በጣሊያን አድርገናል በሚል ለባለስልጣናቱ እንደዋሸች አስታውቀዋል ። ቤተሰቦቿ ነዋሪነታቸው በምዕራብ ጀርመን ሳውየርላንድ አካባቢ ነው። በዚህ ቤት ውስጥ እናት እና አያቷቿ አንዲትን ህፃን ለሰባት ዓመታት ያህል ከአለም እንድትነጠል ማድረጋቸውን አቃብያነ ህግ ተናግረዋል። በዚህ ወቅትም ይህች ህፃን ከማንም ሰው ጋር ግንኙት እንዳይኖራት፣ ትምህርት ቤት እንዳትሄድና ውጭም እንዳትወጣ ተከልክላለች ብለዋል። እስካሁን ባለው አካላዊ ጥቃት ደርሶባታል ወይም የተመጣጠነ ምግብ እጥረት ስለመግጠሙ ምንም አይነት መረጃ የለም ብለዋል። የአካባቢው የህጻናት ደህንነት ክፍል ኃላፊ ሚካኤል ፋርበር ማንበብ እና አንዳንድ ቀላል ሂሳቦችን መስራት እንደምትችል ተናግረዋል። ሆኖም በእለት ተእለት ያሉ ቀላል ተግባራት ለማከናወን እንደምትፈተን አስታውቀዋል። በአሁኑ ወቅት ህጻኗ በልጆች የስነ ልቦና ባለሙያዎች እንክብካቤ ውስጥ ትገኛለች። የጀርመን ብሔራዊ የሕፃናት ጥበቃ ማኅበር ባለሙያ ለጀርመን መገናኛ ብዙኃን እንደተናገሩት “ በአሁኑ ወቅት ለህጻኗ ዓለም ተገለባብጣባታለች። በሌላ ፕላኔት ላይ የመሆን ያህል ነው የሚሰማት” ብለዋል። አቃብያነ ህግ እናትና አያቶች ለምን ህጻኗን ከአለም መነጠል እንዳስፈለጋቸው ምክንያታቸውን ለማወቅ እየሞከሩ ነው። ነገር ግን እናቷ ህፃኗን ከአባቷ ለማራቅ በሚል እንደሆነ ተገምቷል። ልጅቷ ከመወለዷ ትንሽ ቀደም ብሎ ነው ግለሰቧ ከልጇ አባት ጋር የተለያየችው። ልጁን እንዳያይ በመከልከሏም አባትየው ጉዳዩን ወደ ቤተሰብ ፍርድ ቤት ወስዶት ነበር። ፍርድ ቤቱም በአውሮፓውያኑ 2012 በጋራ እንዲያሳድጉ ብያኔ አስተላለፈ። ነገር ግን የጀርመን ባለስልጣናትም ሆነ አባትየው ያምኑ የነበረው እናት ልጇን ይዛ አገር ለቃ እንደወጣች ነበር። እሷም በአውሮፓውያኑ 2015 ወደ ጣሊያን እንደሄደች ለባለስልጣናቱ አሳውቃ ነበር። አገር ለቃ እንዳልወጣችና ልጇንም ይዛ ወላጆቿ ቤት እንደገባችና እነሱም በዚህ ተባባሪ እንደሆኑ ተገልጿል። በዚህች 24 ሺህ ነዋሪዎች ባሉባት ትንሽ ከተማ ውስጥ አንዲት ህጻን ለረጅም ጊዜ ተደብቃ መቆየቷ በአካባቢው ግርምታን ፈጥሯል። ባለስልጣናቱ በቅርብ ዓመታት ጥቆማ ስለደረሳቸውም ነበር ምርመራ የከፈቱት፤ ሆኖም በወቅቱ ተጨባጭ መረጃ ማግኘት አልቻሉም ነበር። ባለፈው ዓመት ሰኔ ላይ አንድ ባልና ሚስት ህፃኗን እንዳዩዋት ለመርማሪዎች ማሳወቃቸውንም ተከትሎ በተከፈተ ምርመራ እናቲቱ ጣሊያን ኖራ እንደማታውቅ ደረሱበት። የህጻኗ እናት እና አያቶች በህገወጥ እስር እና ግፍ በማድረስ ወንጀል ተጠርጥረው እየተመረመሩ ይገኛሉ። አቃቤ ህግ እናት እስከ 10 አመት እስራት ልትቀጣ እንደምትችል ቢያሳውቅም እስካሁን ግን ምንም አይነት ክስ አልተመሰረተም።', 'hyperparam': {'dataset': 'wura', 'date': None, 'dump': None, 'file_path': None, 'id': 'wura_amh_Ethi_0', 'language': 'amh_Ethi', 'language_score': None, 'language_script': None, 'minhash_cluster_size': None, 'top_langs': None, 'url': None}, 'dataset_origin': 'wura'}\n",
      "{'text': \"ሱመር\\nሱመር (አካድኛ፦ ሹመር፣ ግብጽኛ፦ ሳንጋር፣ ዕብራይስጥ፦ ሰናዖር) በጥንታዊ መካከለኛ ምሥራቅ አለም በቅድሚያ ሥልጣኔ የሚባል ኹኔታ የደረሰለት አገር ነበር። ዘመኑ ከታሪካዊ መዝገቦች መነሻ ጀምሮ ይታወቃል። 'ሱመራዊ' ማለት ሱመርኛ የቻሉ ወገኖች ሁሉ ነው። ከጥንታዊ ግብፅ እና ከሕንዶስ ወንዝ ሸለቆ ሥልጣኔ ጋራ ከሁሉ መጀመርያ ሥልጣኔ ካሳዩት አገሮች አንዱ ነው ይባላል።\\nስም [ለማስተካከል]\\nታሪክ [ለማስተካከል]\\nጥንታዊው የሱመራውያን ነገሥታት ዝርዝር የያንዳንዱን ከተማ ሥርወ መንግሥት ይዘርዝራል። ከነዚህም በመግቢያው ያላቸው ነገሥታት ከማየ አይህ በፊት እንደ ነገሡ ይላል። እነዚህ ስሞች ትውፊታዊ ሊሆኑ ይችላል። ከሌላ አፈ ታሪክ ምንጭ የታወቀው በዝርዝሩ የተገኘ ስም የኪሽ ንጉሥ ኤታና ነው። ከዚያ በኋላ የኡሩክ መጀመርያ ነገሥታት ኤንመርካር፣ ሉጋልባንዳ፣ ዱሙዚድና ጊልጋመሽ ሁላቸው ከሌሎች ትውፊቶች ይታወቃሉ። ከሥነ ቅርስ የታወቀው መጀመርያ ስም ኤላምን ያሸነፈው የኪሽ ንጉሥ ኤንመባራገሲ ነው። እሱ ደግሞ የጊልጋሜሽ ትውፊት በተባለ ጽሑፍ ስለሚገኝ ይህ ለጊልጋሜሽ ታሪካዊ ሕልውና ምናልባትነት መስጠቱ ይታመናል።\\nየኡሩክ ንጉሥ ኤንሻኩሻና ሐማዚን፣ አካድን፣ ኪሽንና ኒፑርን በማሸነፉ መጀመርያ 'የሱመርና የአካድ ንጉሥ' የተባለ ነበረ። ከዚህ ቀጥሎ የላጋሽ ንጉሥ ኤ-አና-ቱም ሱመርን ሁሉ ኤላምንም በከፊል አሸነፈ። ከተገኙ ጽላቶች መሠረት፣ የሱ መንግሥት አደራረግ ተገዥ ወገኖችን ማስፈራራትና ግፍ መሆኑን ልንገምት እንችላለን። እሱ እንደ ሞተ፣ ግዛቱ ለጊዜው በየከተማው ተከፋፈለ።\\nላዕላይነቱ አሁን እንደ «ቅዱስ የሮማ መንግሥት ንጉስ» የመሰለ ማዕረግ ሆኖ ነበር። ከከተማ-አገሮቹ የማሪ ንጉስ ማዕረጉን ከያዘ፣ ከዚህ ወደ አክሻክ ንጉሥ ተዛወረ። የአክሻክ ንጉስ ፑዙር-ኒራሕ ክፉ ሆኖ፣ የሱመር ቄሳውንት መንግሥቱን ለአንዲት ሴት ባለ ጠጅ ቤት ለኩግ-ባው ንግሥት ሆና እንዳሸለሙላት የሚል ሰነድ አለ። በዚህም ዘመን በላጋሽ፣ ከአንድ ክፉ ንጉሥ ሉጋላንዳ በኋላ፣ የላጋሽ ንጉሥ ኡሩካጊና የራሱን ሕገጋት አውጥቶ ለድኆች ማሻሻል አደረገ፤ በተጨማሪ 1 ሚስት ቀድሞ ብዙ ባሎችን የምታገባበትን ልማድ ከለከለ።\\nየአካድ መንግሥት በጉታውያን ዕጅ ከወደቀ በኋላ የላጋሽ ንጉሥ ጉዴአ ተነሣ። ከዚያ በኋላ የኡር ንጉስ ኡር-ናሙ ላጋሽን አሸንፎ ላይኛነቱን ያዘ። እሱ በተለይ የኡር-ናሙ ሕገጋት ስለ ተባለው ፍትኅ ይታወሳል። በዚህ ወቅት ብዙ ሴማዊ ቋንቋ ያላችው አሞራውያን ወገኖች ወደ ሜስጶጦምያ ስለ ገቡ፣ የሱመርኛ ጥቅም ቀስ በቀስ ቀነሰ። ሆኖም ሱመርኛ በትምህርት ቤትና በሃይማኖት ረገድ ይቀጠል ነበር።\\nይህ የኡር መንግሥት እስከ 2012 ክ.በ. ኤላማውያን እስከ ወረሩት ድረስ ቆየ። የዚያ ውጤት አሞራውያን ለራሳቸው ከተሞች ያዙና የሱመር ሃይል ጠፋ። ከነዚህ ከተሞች መካከል ባቢሎን በንጉስዋ ሃሙራቢ ወቅት ላይኛነቱን ለረጅም ወራት መሰረተች።\", 'hyperparam': {'dataset': 'fw2', 'date': '2013-05-24T10:47:09Z', 'dump': 'CC-MAIN-2013-20', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368704590423/warc/CC-MAIN-20130516114310-00020-ip-10-60-113-184.ec2.internal.warc.gz', 'id': '<urn:uuid:a282dc3d-0a15-4e6d-91ab-2d1f721c72ff>', 'language': 'amh_Ethi', 'language_score': 0.9999779462814331, 'language_script': 'Ethi', 'minhash_cluster_size': 66, 'top_langs': '{\"amh_Ethi_score\": 0.9999779462814331}', 'url': 'http://am.wikipedia.org/wiki/%E1%88%B1%E1%88%98%E1%88%AD'}, 'dataset_origin': 'fw2'}\n"
     ]
    }
   ],
   "source": [
    "# Loading a single language\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "configs = get_dataset_config_names(\"Tiany1/multilingualPretrainDataset\")\n",
    "\n",
    "print(\"Available languages:\", configs)\n",
    "\n",
    "ds_aeb = load_dataset(\"Tiany1/multilingualPretrainDataset\", \"amh_Ethi\")\n",
    "\n",
    "print(ds_aeb)\n",
    "\n",
    "print(ds_aeb[\"wura_passageLevel\"][0])        # first record from the wurapassage level split\n",
    "\n",
    "print(ds_aeb[\"fineweb2\"][0])        # first record from the fineweb2 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e3142cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating wura_passageLevel split: 2336199 examples [00:03, 682650.62 examples/s]\n",
      "Generating wura_documentLevel split: 1378555 examples [00:04, 328715.99 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"fineweb2\". Should be one of ['wura_passageLevel', 'wura_documentLevel'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# only load the fineweb2 split for English-Latin\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m fw2_eng = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTiany1/multilingualPretrainDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meng_Latn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfineweb2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(fw2_eng)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/load.py:2096\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2096\u001b[39m ds = \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_infos:\n\u001b[32m   2098\u001b[39m     builder_instance._save_infos()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/builder.py:1127\u001b[39m, in \u001b[36mDatasetBuilder.as_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, in_memory)\u001b[39m\n\u001b[32m   1124\u001b[39m verification_mode = VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS)\n\u001b[32m   1126\u001b[39m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m datasets = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1139\u001b[39m     datasets = DatasetDict(datasets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/utils/py_utils.py:494\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    493\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    496\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/builder.py:1157\u001b[39m, in \u001b[36mDatasetBuilder._build_single_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, in_memory)\u001b[39m\n\u001b[32m   1154\u001b[39m     split = Split(split)\n\u001b[32m   1156\u001b[39m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1157\u001b[39m ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post_processing_resources(split).values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/builder.py:1231\u001b[39m, in \u001b[36mDatasetBuilder._as_dataset\u001b[39m\u001b[34m(self, split, in_memory)\u001b[39m\n\u001b[32m   1229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_legacy_cache():\n\u001b[32m   1230\u001b[39m     dataset_name = \u001b[38;5;28mself\u001b[39m.name\n\u001b[32m-> \u001b[39m\u001b[32m1231\u001b[39m dataset_kwargs = \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1237\u001b[39m fingerprint = \u001b[38;5;28mself\u001b[39m._get_dataset_fingerprint(split)\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint=fingerprint, **dataset_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/arrow_reader.py:248\u001b[39m, in \u001b[36mBaseReader.read\u001b[39m\u001b[34m(self, name, instructions, split_infos, in_memory)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    229\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m     in_memory=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    233\u001b[39m ):\n\u001b[32m    234\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m \u001b[33;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[32m    250\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInstruction \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m corresponds to no data!\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/arrow_reader.py:221\u001b[39m, in \u001b[36mBaseReader.get_file_instructions\u001b[39m\u001b[34m(self, name, instruction, split_infos)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[32m    220\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     file_instructions = \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     files = file_instructions.file_instructions\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/arrow_reader.py:130\u001b[39m, in \u001b[36mmake_file_instructions\u001b[39m\u001b[34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[39m\n\u001b[32m    128\u001b[39m     instruction = ReadInstruction.from_spec(instruction)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m absolute_instructions = \u001b[43minstruction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[32m    133\u001b[39m file_instructions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/arrow_reader.py:620\u001b[39m, in \u001b[36mReadInstruction.to_absolute\u001b[39m\u001b[34m(self, name2len)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[32m    609\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[32m    610\u001b[39m \n\u001b[32m    611\u001b[39m \u001b[33;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m \u001b[33;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._relative_instructions]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github_projects/data_pretrain/another_venv/lib/python3.13/site-packages/datasets/arrow_reader.py:437\u001b[39m, in \u001b[36m_rel_to_abs_instr\u001b[39m\u001b[34m(rel_instr, name2len)\u001b[39m\n\u001b[32m    435\u001b[39m split = rel_instr.splitname\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown split \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    438\u001b[39m num_examples = name2len[split]\n\u001b[32m    439\u001b[39m from_ = rel_instr.from_\n",
      "\u001b[31mValueError\u001b[39m: Unknown split \"fineweb2\". Should be one of ['wura_passageLevel', 'wura_documentLevel']."
     ]
    }
   ],
   "source": [
    "# Load a single langauge with specified splits\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# only load the fineweb2 split for English-Latin\n",
    "fw2_eng = load_dataset(\n",
    "    \"Tiany1/multilingualPretrainDataset\",\n",
    "    name=\"eng_Latn\",\n",
    "    split=\"fineweb2\"\n",
    ")\n",
    "print(fw2_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee755ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple languages (all splits) in a loop\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "langs = [\"eng_Latn\", \"fra_Latn\", \"por_Latn\"]\n",
    "all_ds = {}\n",
    "\n",
    "for lang in langs:\n",
    "    all_ds[lang] = load_dataset(\"Tiany1/multilingualPretrainDataset\", name=lang)\n",
    "\n",
    "# now all_ds[\"fra_Latn\"][\"wura_passageLevel\"] etc. are available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b67852",
   "metadata": {},
   "source": [
    "## Creating the Dataset_Type hyperparam \n",
    "\n",
    "This is to keep track and load exactly the dataset type I want in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4945d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 12:00:04,552 - INFO - Scanning language folder: eng_Latn\n",
      "2025-05-14 12:00:04,554 - INFO - eng_Latn_0008_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:04,555 - INFO - eng_Latn_0001_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:04,556 - INFO - eng_Latn_0005_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,452 - INFO - eng_Latn_0002_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,457 - INFO - eng_Latn_0006_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,459 - INFO - eng_wura.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,460 - INFO - eng_Latn_0010_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,462 - INFO - eng_Latn_0007_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,463 - INFO - eng_Latn_0003_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,464 - INFO - eng_Latn_0004_fwEdu.parquet already has dataset_origin; skipping\n",
      "2025-05-14 12:00:10,466 - INFO - eng_Latn_0009_fwEdu.parquet already has dataset_origin; skipping\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def infer_origin(stem: str) -> str | None:\n",
    "    stem = stem.lower()\n",
    "    if \"fw2\" in stem:\n",
    "        return \"fw2\"\n",
    "    if \"documentlevel\" in stem:\n",
    "        return \"wuraDocumentLevel\"\n",
    "    if \"fwedu\" in stem:\n",
    "        return \"fwedu\"\n",
    "    if \"ml400\" in stem:\n",
    "        return \"madlad400\"\n",
    "    if \"wura\" in stem:\n",
    "        return \"wura\"\n",
    "    if \"extra\" in stem:\n",
    "        return \"extra\"\n",
    "    return None\n",
    "\n",
    "def process_file_in_chunks(pq_path: Path, origin: str, batch_size: int = 50_000):\n",
    "    \"\"\"\n",
    "    Read pq_path in batches, add a constant `dataset_origin` column,\n",
    "    and overwrite the file in a memory-safe way.\n",
    "    \"\"\"\n",
    "    # Open source file\n",
    "    parquet_file = pq.ParquetFile(pq_path)\n",
    "\n",
    "    # Extend schema with new string column\n",
    "    new_schema = parquet_file.schema_arrow.append(\n",
    "        pa.field(\"dataset_origin\", pa.string())\n",
    "    )\n",
    "\n",
    "    tmp_path = pq_path.with_suffix(\".tmp.parquet\")\n",
    "    writer = pq.ParquetWriter(tmp_path, new_schema, compression=\"snappy\")\n",
    "\n",
    "    # Stream through row‐groups / batches\n",
    "    for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        table = pa.Table.from_batches([batch], schema=parquet_file.schema_arrow)\n",
    "        # constant column with `origin`\n",
    "        origin_col = pa.array([origin] * table.num_rows)\n",
    "        table = table.append_column(\"dataset_origin\", origin_col)\n",
    "        writer.write_table(table)\n",
    "\n",
    "    writer.close()\n",
    "    tmp_path.replace(pq_path)\n",
    "\n",
    "\n",
    "def add_origin_column(root_dir: str = \"preprocessed_data\"):\n",
    "    root = Path(root_dir)\n",
    "    if not root.is_dir():\n",
    "        logger.error(f\"Root directory {root_dir!r} does not exist or is not a folder.\")\n",
    "        return\n",
    "\n",
    "    for lang_dir in root.iterdir():\n",
    "        if not lang_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Scanning language folder: {lang_dir.name}\")\n",
    "        for pq_file in lang_dir.glob(\"*.parquet\"):\n",
    "            # Detect existing column\n",
    "            try:\n",
    "                schema = pq.ParquetFile(pq_file).schema_arrow\n",
    "                if \"dataset_origin\" in schema.names:\n",
    "                    logger.info(f\"{pq_file.name} already has dataset_origin; skipping\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read schema for {pq_file.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            origin = infer_origin(pq_file.stem)\n",
    "            if origin is None:\n",
    "                logger.warning(f\"Could not infer origin for {pq_file.name}; skipping\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Processing {pq_file.name} in chunks (origin={origin})\")\n",
    "            try:\n",
    "                process_file_in_chunks(pq_file, origin)\n",
    "                logger.info(f\"✅ Updated {pq_file.name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {pq_file.name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    add_origin_column(\"preprocessed_data/eng_Latn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53605cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: {'text': Value(dtype='string', id=None), 'hyperparam': {'dataset': Value(dtype='string', id=None), 'date': Value(dtype='string', id=None), 'dump': Value(dtype='string', id=None), 'file_path': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'language_score': Value(dtype='float64', id=None), 'language_script': Value(dtype='string', id=None), 'minhash_cluster_size': Value(dtype='int64', id=None), 'top_langs': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None)}, 'dataset_origin': Value(dtype='string', id=None)}\n",
      "{'text': 'Wěmàxòkplé sùpípù tɔn\\nSùpípù ɔ tìtómɛ nù jíjlá tɔn ɖê minirézo kéjé bó lɛ vlɔnkán nú bíbló gblògblòjí uZine tɔn ɔ. Mì sɔ kɛ mí ɖò àcɛ wàlɔ GPl tɔn mɛ. Mī zé bò zán bó dó bló gblògblòjí lɛ̂ ná : é síwú nyí mì tɔn ɖé sú kàbí gbɛtá ɖé tɔn, kàbí tòxóɖɔgbɛ ɖé tɔn, àló àjɔwíwá wú.\\nGblògblòjí é lɔ̂, é wɛ nyí Wěmàxòkplé sùpípù tɔn ɖò tájí. Mí ná lɛ mɔ àlɔwlí gégé ɖê ná ɖɔn mî yì gblògblòjí ɖê jí kplékplé lɛ̂ nɔ tîn ɖè ɔ\\nXó ɖébú ɖé jló mi ɔ mi síwú kanbyɔ ɖò Kplékplé é lɔ mɛ. Fíwɛ mi ná mɔ àlɔdó lɛ bí ɖè.\\n11931 Sites Web', 'hyperparam': {'dataset': 'fw2', 'date': '2013-05-20T08:52:06Z', 'dump': 'CC-MAIN-2013-20', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368698646863/warc/CC-MAIN-20130516100406-00073-ip-10-60-113-184.ec2.internal.warc.gz', 'id': '<urn:uuid:2d83a636-c85a-4c84-8ea3-0e4b38991be3>', 'language': 'fon_Latn', 'language_score': 0.9999923706054688, 'language_script': 'Latn', 'minhash_cluster_size': 41, 'top_langs': '{\"fon_Latn_score\": 0.9999923706054688}', 'url': 'http://www.spip.net/fon_rubrique357.html'}, 'dataset_origin': 'fw2'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"preprocessed_data/fon_Latn/fon_Latn_0001_fw2.parquet\"},\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"Available columns:\", dataset.features)\n",
    "\n",
    "print(next(iter(dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32af8d",
   "metadata": {},
   "source": [
    "# Double check for bad files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597f1d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Found invalid parquet files:\n",
      "   - preprocessed_data/por_Latn/por_Latn_0004_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/por_Latn/por_Latn_0011_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/por_Latn/por_Latn_0002_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/por_Latn/por_Latn_0009_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/fra_Latn/fra_Latn_0009_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/fra_Latn/fra_Latn_0007_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n",
      "   - preprocessed_data/afr_Latn/afr_Latn_0001_fw2.parquet  (TimeoutError: [Errno 60] Error reading bytes from file. Detail: [errno 60] Operation timed out)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def find_bad_parquet_files(preproc_root: str = \"preprocessed_data\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Scan all .parquet files under preproc_root and return a list of paths\n",
    "    that are either empty or that PyArrow rejects as invalid Parquet.\n",
    "    \"\"\"\n",
    "    bad_files = []\n",
    "    root = Path(preproc_root)\n",
    "    for pq_path in root.rglob(\"*.parquet\"):\n",
    "        # 1) skip empty files\n",
    "        if pq_path.stat().st_size == 0:\n",
    "            bad_files.append(str(pq_path) + \"  (empty file)\")\n",
    "            continue\n",
    "\n",
    "        # 2) try to read metadata\n",
    "        try:\n",
    "            _ = pq.ParquetFile(pq_path).metadata\n",
    "        except Exception as e:\n",
    "            bad_files.append(f\"{pq_path}  ({type(e).__name__}: {e})\")\n",
    "\n",
    "    return bad_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bad = find_bad_parquet_files(\"preprocessed_data\")\n",
    "    if not bad:\n",
    "        print(\"✅ All .parquet files look valid.\")\n",
    "    else:\n",
    "        print(\"❌ Found invalid parquet files:\")\n",
    "        for entry in bad:\n",
    "            print(\"   -\", entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828da92",
   "metadata": {},
   "source": [
    "# Create the Huggingface Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
