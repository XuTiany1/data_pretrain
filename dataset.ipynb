{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925eefef",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4d2be",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3061a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/network/scratch/x/xut/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(os.environ[\"HF_HOME\"], \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(os.environ[\"HF_HOME\"], \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ee113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/x/xut/.conda/envs/reasoners/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mila/x/xut/.conda/envs/reasoners/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datatrove.pipeline.readers import ParquetReader\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1db57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_web2_labels = ['aeb_Arab', \n",
    "                    'afr_Latn', \n",
    "                    'amh_Ethi', \n",
    "                    'arz_Arab', \n",
    "                    'bam_Latn', \n",
    "                    'bem_Latn', \n",
    "                    'cjk_Latn', \n",
    "                    'dyu_Latn', \n",
    "                    'gaz_Latn', \n",
    "                    'ibo_Latn', \n",
    "                    'kab_Latn', \n",
    "                    'kam_Latn', \n",
    "                    'kbp_Latn', \n",
    "                    'kin_Latn', \n",
    "                    'kmb_Latn', \n",
    "                    'knc_Arab', \n",
    "                    'knc_Latn', \n",
    "                    'lin_Latn', \n",
    "                    'lug_Latn', \n",
    "                    'luo_Latn', \n",
    "                    'nus_Latn', \n",
    "                    'plt_Latn', \n",
    "                    'run_Latn', \n",
    "                    'sag_Latn', \n",
    "                    'sna_Latn', \n",
    "                    'sot_Latn', \n",
    "                    'ssw_Latn', \n",
    "                    'swc_Latn', \n",
    "                    'taq_Tfng', \n",
    "                    'tir_Ethi', \n",
    "                    'tsn_Latn', \n",
    "                    'twi_Latn', \n",
    "                    'tzm_Tfng', \n",
    "                    'umb_Latn', \n",
    "                    'xho_Latn', \n",
    "                    'yor_Latn', \n",
    "                    'zul_Latn',\n",
    "                    'spa_Latn',\n",
    "                    'fra_Latn',\n",
    "                    'por_Latn'\n",
    "                    ]\n",
    "\n",
    "afri_mgsm_langs = {\n",
    "    \"amh_Ethi\": \"amh\",\n",
    "    \"ewe_Latn\": \"ewe\",\n",
    "    \"gaz_Latn\": \"orm\",  \n",
    "    \"hau_Latn\": \"hau\",\n",
    "    \"kin_Latn\": \"kin\",\n",
    "    \"lin_Latn\": \"lin\",\n",
    "    \"lug_Latn\": \"lug\",\n",
    "    \"sna_Latn\": \"sna\",\n",
    "    \"swc_Latn\": \"swa\",\n",
    "    \"twi_Latn\": \"twi\",\n",
    "    \"wol_Latn\": \"wol\",\n",
    "    \"xho_Latn\": \"xho\",\n",
    "    \"yor_Latn\": \"yor\",\n",
    "    \"zul_Latn\": \"zul\"\n",
    "}\n",
    "\n",
    "\n",
    "mgsm_langs = {\n",
    "    \"eng_Latn\": \"en\",\n",
    "    \"spa_Latn\": \"es\",\n",
    "    \"fra_Latn\": \"fr\"\n",
    "}\n",
    "\n",
    "\n",
    "wura_langs = {\n",
    "        \"afr_Latn\": \"af\",\n",
    "        \"amh_Ethi\": \"am\",\n",
    "        \"arz_Arab\": \"ar\",\n",
    "        \"hau_Latn\": \"ha\",\n",
    "        \"ibo_Latn\": \"ig\",\n",
    "        \"kin_Latn\": \"ki\",\n",
    "        \"plt_Latn\": \"mg\",\n",
    "        \"gaz_Latn\": \"or\",\n",
    "        \"som_Latn\": \"sm\",\n",
    "        \"sna_Latn\": \"sn\",\n",
    "        \"sot_Latn\": \"st\",\n",
    "        \"swc_Latn\": \"sw\",\n",
    "        \"tir_Ethi\": \"ti\",\n",
    "        \"xho_Latn\": \"xh\",\n",
    "        \"yor_Latn\": \"yo\",\n",
    "        \"zul_Latn\": \"zu\",\n",
    "}\n",
    "\n",
    "\n",
    "madlad_langs = {\n",
    "    \"afr_Latn\": \"af\",\n",
    "    \"aka_Latn\": \"ak\",\n",
    "    \"amh_Ethi\": \"am\",\n",
    "    \"bam_Latn\": \"bm\",\n",
    "    \"dik_Latn\": \"din\",\n",
    "    \"dyu_Latn\": \"dyu\",\n",
    "    \"ewe_Latn\": \"ee\",\n",
    "    \"fon_Latn\": \"fon\",\n",
    "    \"fuv_Latn\": \"ff\",\n",
    "    \"gaz_Latn\": \"om\",   \n",
    "    \"hau_Latn\": \"ha\",  \n",
    "    \"ibo_Latn\": \"ig\",   \n",
    "    \"kbp_Latn\": \"kbp\",\n",
    "    \"kin_Latn\": \"rw\",\n",
    "    \"kmb_Latn\": \"kmb\",\n",
    "    \"kon_Latn\": \"kg\",\n",
    "    \"lin_Latn\": \"ln\",\n",
    "    \"lug_Latn\": \"lg\",\n",
    "    \"run_Latn\": \"rn\",\n",
    "    \"sag_Latn\": \"sg\",\n",
    "    \"sna_Latn\": \"sn\",\n",
    "    \"som_Latn\": \"so\",\n",
    "    \"sot_Latn\": \"st\",\n",
    "    \"ssw_Latn\": \"ss\",\n",
    "    \"swc_Latn\": \"sw\",\n",
    "    \"tir_Ethi\": \"ti\",\n",
    "    \"tsn_Latn\": \"tn\",\n",
    "    \"tso_Latn\": \"ts\",\n",
    "    \"tzm_Tfng\": \"ber\",\n",
    "    \"wol_Latn\": \"wo\",\n",
    "    \"xho_Latn\": \"xh\",\n",
    "    \"yor_Latn\": \"yo\",\n",
    "    \"zul_Latn\": \"zu\"\n",
    "}\n",
    "\n",
    "all_african_language_list = [\n",
    "    'aeb_Arab',\n",
    "    'afr_Latn',\n",
    "    'aka_Latn',\n",
    "    'amh_Ethi',\n",
    "    'ary_Arab',\n",
    "    'arz_Arab',\n",
    "    'bam_Latn',\n",
    "    'bem_Latn',\n",
    "    'cjk_Latn',\n",
    "    'dik_Latn',\n",
    "    'dyu_Latn',\n",
    "    'ewe_Latn',\n",
    "    'fon_Latn',\n",
    "    'fuv_Latn',\n",
    "    'gaz_Latn',\n",
    "    'hau_Latn',\n",
    "    'ibo_Latn',\n",
    "    'kab_Latn',\n",
    "    'kam_Latn',\n",
    "    'kbp_Latn',\n",
    "    'kea_Latn',\n",
    "    'kik_Tatn',\n",
    "    'kin_Latn',\n",
    "    'kmb_Latn',\n",
    "    'knc_Arab',\n",
    "    'knc_Latn',\n",
    "    'kon_Latn',\n",
    "    'lin_Latn',\n",
    "    'lua_Latn',\n",
    "    'lug_Latn',\n",
    "    'luo_Latn',\n",
    "    'Mos_Latn',\n",
    "    'nqo_Nkoo',\n",
    "    'nso_Latn',\n",
    "    'nus_Latn',\n",
    "    'nya_Latn',\n",
    "    'plt_Latn',\n",
    "    'run_Latn',\n",
    "    'sag_Latn',\n",
    "    'sna_Latn',\n",
    "    'som_Latn',\n",
    "    'sot_Latn',\n",
    "    'ssw_Latn',\n",
    "    'swc_Latn',\n",
    "    'taq_Latn',\n",
    "    'taq_Tfng',\n",
    "    'tir_Ethi',\n",
    "    'tsn_Latn',\n",
    "    'tso_Latn',\n",
    "    'tum_Latn',\n",
    "    'twi_Latn',\n",
    "    'tzm_Tfng',\n",
    "    'umb_Latn',\n",
    "    'wol_Latn',\n",
    "    'xho_Latn',\n",
    "    'yor_Latn',\n",
    "    'zul_Latn',\n",
    "]\n",
    "\n",
    "high_resource_lang = {\n",
    "    'eng_Latn',\n",
    "    'spa_Latn',\n",
    "    'fra_Latn',\n",
    "    'por_Latn'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90609c30",
   "metadata": {},
   "source": [
    "# Fineweb 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390aa070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeb_Arab: ../../scratch/data/data_pretrain/fineweb2/aeb_Arab_fw2.jsonl already exists, skipping.\n",
      "afr_Latn: ../../scratch/data/data_pretrain/fineweb2/afr_Latn_fw2.jsonl already exists, skipping.\n",
      "amh_Ethi: ../../scratch/data/data_pretrain/fineweb2/amh_Ethi_fw2.jsonl already exists, skipping.\n",
      "arz_Arab: ../../scratch/data/data_pretrain/fineweb2/arz_Arab_fw2.jsonl already exists, skipping.\n",
      "bam_Latn: ../../scratch/data/data_pretrain/fineweb2/bam_Latn_fw2.jsonl already exists, skipping.\n",
      "bem_Latn: ../../scratch/data/data_pretrain/fineweb2/bem_Latn_fw2.jsonl already exists, skipping.\n",
      "cjk_Latn: ../../scratch/data/data_pretrain/fineweb2/cjk_Latn_fw2.jsonl already exists, skipping.\n",
      "dyu_Latn: ../../scratch/data/data_pretrain/fineweb2/dyu_Latn_fw2.jsonl already exists, skipping.\n",
      "gaz_Latn: ../../scratch/data/data_pretrain/fineweb2/gaz_Latn_fw2.jsonl already exists, skipping.\n",
      "ibo_Latn: ../../scratch/data/data_pretrain/fineweb2/ibo_Latn_fw2.jsonl already exists, skipping.\n",
      "kab_Latn: ../../scratch/data/data_pretrain/fineweb2/kab_Latn_fw2.jsonl already exists, skipping.\n",
      "kam_Latn: ../../scratch/data/data_pretrain/fineweb2/kam_Latn_fw2.jsonl already exists, skipping.\n",
      "kbp_Latn: ../../scratch/data/data_pretrain/fineweb2/kbp_Latn_fw2.jsonl already exists, skipping.\n",
      "kin_Latn: ../../scratch/data/data_pretrain/fineweb2/kin_Latn_fw2.jsonl already exists, skipping.\n",
      "kmb_Latn: ../../scratch/data/data_pretrain/fineweb2/kmb_Latn_fw2.jsonl already exists, skipping.\n",
      "knc_Arab: ../../scratch/data/data_pretrain/fineweb2/knc_Arab_fw2.jsonl already exists, skipping.\n",
      "knc_Latn: ../../scratch/data/data_pretrain/fineweb2/knc_Latn_fw2.jsonl already exists, skipping.\n",
      "lin_Latn: ../../scratch/data/data_pretrain/fineweb2/lin_Latn_fw2.jsonl already exists, skipping.\n",
      "lug_Latn: ../../scratch/data/data_pretrain/fineweb2/lug_Latn_fw2.jsonl already exists, skipping.\n",
      "luo_Latn: ../../scratch/data/data_pretrain/fineweb2/luo_Latn_fw2.jsonl already exists, skipping.\n",
      "nus_Latn: ../../scratch/data/data_pretrain/fineweb2/nus_Latn_fw2.jsonl already exists, skipping.\n",
      "plt_Latn: ../../scratch/data/data_pretrain/fineweb2/plt_Latn_fw2.jsonl already exists, skipping.\n",
      "run_Latn: ../../scratch/data/data_pretrain/fineweb2/run_Latn_fw2.jsonl already exists, skipping.\n",
      "sag_Latn: ../../scratch/data/data_pretrain/fineweb2/sag_Latn_fw2.jsonl already exists, skipping.\n",
      "sna_Latn: ../../scratch/data/data_pretrain/fineweb2/sna_Latn_fw2.jsonl already exists, skipping.\n",
      "sot_Latn: ../../scratch/data/data_pretrain/fineweb2/sot_Latn_fw2.jsonl already exists, skipping.\n",
      "ssw_Latn: ../../scratch/data/data_pretrain/fineweb2/ssw_Latn_fw2.jsonl already exists, skipping.\n",
      "swc_Latn: ../../scratch/data/data_pretrain/fineweb2/swc_Latn_fw2.jsonl already exists, skipping.\n",
      "taq_Tfng: ../../scratch/data/data_pretrain/fineweb2/taq_Tfng_fw2.jsonl already exists, skipping.\n",
      "tir_Ethi: ../../scratch/data/data_pretrain/fineweb2/tir_Ethi_fw2.jsonl already exists, skipping.\n",
      "tsn_Latn: ../../scratch/data/data_pretrain/fineweb2/tsn_Latn_fw2.jsonl already exists, skipping.\n",
      "twi_Latn: ../../scratch/data/data_pretrain/fineweb2/twi_Latn_fw2.jsonl already exists, skipping.\n",
      "tzm_Tfng: ../../scratch/data/data_pretrain/fineweb2/tzm_Tfng_fw2.jsonl already exists, skipping.\n",
      "umb_Latn: ../../scratch/data/data_pretrain/fineweb2/umb_Latn_fw2.jsonl already exists, skipping.\n",
      "xho_Latn: ../../scratch/data/data_pretrain/fineweb2/xho_Latn_fw2.jsonl already exists, skipping.\n",
      "yor_Latn: ../../scratch/data/data_pretrain/fineweb2/yor_Latn_fw2.jsonl already exists, skipping.\n",
      "zul_Latn: ../../scratch/data/data_pretrain/fineweb2/zul_Latn_fw2.jsonl already exists, skipping.\n",
      "spa_Latn: ../../scratch/data/data_pretrain/fineweb2/spa_Latn_fw2.jsonl already exists, skipping.\n",
      "fra_Latn: ../../scratch/data/data_pretrain/fineweb2/fra_Latn_fw2.jsonl already exists, skipping.\n",
      "por_Latn: ../../scratch/data/data_pretrain/fineweb2/por_Latn_fw2.jsonl already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Read token\n",
    "with open(\"/network/scratch/x/xut/hf_cache/token\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "# Hyperparam\n",
    "output_dir = \"../../scratch/data/data_pretrain/fineweb2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "MAX_TOKENS = 1_000_000_000\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "# Tokenizer (Gemma 2B/7B tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", token=token)\n",
    "\n",
    "# Count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "# Loop through languages\n",
    "for lang_code in fine_web2_labels:\n",
    "    reader = ParquetReader(f\"hf://datasets/HuggingFaceFW/fineweb-2/data/{lang_code}/train\")\n",
    "    output_path = os.path.join(output_dir, f\"{lang_code}_fw2.jsonl\")\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{lang_code}: {output_path} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    token_count = 0\n",
    "    doc_count = 0\n",
    "    buffer = []\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for doc in tqdm(reader(), desc=f\"{lang_code} docs\"):\n",
    "            text = doc.text.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            tokens = count_tokens(text)\n",
    "            token_count += tokens\n",
    "            doc_count += 1\n",
    "            buffer.append(json.dumps({\"text\": text}))\n",
    "\n",
    "            if len(buffer) >= BUFFER_SIZE:\n",
    "                out_file.write(\"\\n\".join(buffer) + \"\\n\")\n",
    "                buffer = []\n",
    "\n",
    "            if token_count >= MAX_TOKENS:\n",
    "                break\n",
    "\n",
    "        # Write remaining documents in buffer\n",
    "        if buffer:\n",
    "            out_file.write(\"\\n\".join(buffer) + \"\\n\")\n",
    "\n",
    "    print(f\"{lang_code}: {doc_count:,} docs, {token_count:,} tokens ➜ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f5a15",
   "metadata": {},
   "source": [
    "# Afri-MGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0edf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 172.73 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 8472.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/amh_Ethi_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1160.85 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 24394.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/ewe_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1168.33 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 23278.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/gaz_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1168.17 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 25916.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/hau_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1214.99 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 32634.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/kin_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 994.09 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 25486.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/lin_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1458.44 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 41044.98 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/lug_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1135.51 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 25387.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/sna_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1191.86 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 22292.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/swc_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1124.70 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 22741.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/twi_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 971.72 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 25451.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/wol_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 973.02 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 29029.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/xho_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1163.31 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 9778.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/yor_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8/8 [00:00<00:00, 1092.48 examples/s]\n",
      "Generating test split: 100%|██████████| 250/250 [00:00<00:00, 26026.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/afrimgsm/zul_Latn_mgsm.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "output_dir = \"../../scratch/data/data_pretrain/afrimgsm\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each mapping\n",
    "for long_code, mgsm_code in afri_mgsm_langs.items():\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{long_code}_mgsm.jsonl\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{long_code}: {output_path} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"masakhane/afrimgsm\", name=mgsm_code, split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {mgsm_code}: {e}\")\n",
    "        continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            # Check which field has the text content and use that\n",
    "            if \"question\" in example and \"answer\" in example:\n",
    "                # For MGSM, you might want to combine question and answer\n",
    "                text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
    "                json.dump({\"text\": text}, f)\n",
    "            elif \"text\" in example:\n",
    "                # If there's already a text field\n",
    "                json.dump({\"text\": example[\"text\"]}, f)\n",
    "            else:\n",
    "                # Dump the entire example as text if you're not sure which field to use\n",
    "                json.dump({\"text\": str(example)}, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a9e43",
   "metadata": {},
   "source": [
    "# MGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d767bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/mgsm/eng_Latn_mgsm.jsonl\n",
      "Saved to ../../scratch/data/data_pretrain/mgsm/spa_Latn_mgsm.jsonl\n",
      "Saved to ../../scratch/data/data_pretrain/mgsm/fra_Latn_mgsm.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "output_dir = \"../../scratch/data/data_pretrain/mgsm\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each mapping\n",
    "for long_code, mgsm_code in mgsm_langs.items():\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{long_code}_mgsm.jsonl\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{long_code}: {output_path} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"juletxara/mgsm\", name=mgsm_code, split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {mgsm_code}: {e}\")\n",
    "        continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            # Check which field has the text content and use that\n",
    "            if \"question\" in example and \"answer\" in example:\n",
    "                # For MGSM, you might want to combine question and answer\n",
    "                text = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
    "                json.dump({\"text\": text}, f)\n",
    "            elif \"text\" in example:\n",
    "                # If there's already a text field\n",
    "                json.dump({\"text\": example[\"text\"]}, f)\n",
    "            else:\n",
    "                # Dump the entire example as text if you're not sure which field to use\n",
    "                json.dump({\"text\": str(example)}, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa7768",
   "metadata": {},
   "source": [
    "# Wura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de86cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇ Downloading WURA for afr_Latn (af)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2390884/2390884 [01:02<00:00, 38140.27 examples/s]\n",
      "Generating eval split: 100%|██████████| 265117/265117 [00:06<00:00, 38760.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/afr_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for amh_Ethi (am)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 291026/291026 [00:11<00:00, 24287.75 examples/s]\n",
      "Generating eval split: 100%|██████████| 32307/32307 [00:01<00:00, 27808.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/amh_Ethi_wura.jsonl\n",
      "⬇ Downloading WURA for arz_Arab (ar)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1116034/1116034 [00:02<00:00, 464460.98 examples/s]\n",
      "Generating eval split: 100%|██████████| 124808/124808 [00:00<00:00, 561581.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/arz_Arab_wura.jsonl\n",
      "⬇ Downloading WURA for hau_Latn (ha)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 565471/565471 [00:08<00:00, 67807.93 examples/s] \n",
      "Generating eval split: 100%|██████████| 63067/63067 [00:00<00:00, 68103.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/hau_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for ibo_Latn (ig)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 121421/121421 [00:06<00:00, 20210.45 examples/s]\n",
      "Generating eval split: 100%|██████████| 13899/13899 [00:00<00:00, 45357.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/ibo_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for kin_Latn (ki)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 61485/61485 [00:00<00:00, 83720.99 examples/s] \n",
      "Generating eval split: 100%|██████████| 6902/6902 [00:00<00:00, 97443.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/kin_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for plt_Latn (mg)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 355390/355390 [00:06<00:00, 57410.95 examples/s] \n",
      "Generating eval split: 100%|██████████| 39314/39314 [00:00<00:00, 58818.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/plt_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for gaz_Latn (or)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 37280/37280 [00:00<00:00, 69197.13 examples/s]\n",
      "Generating eval split: 100%|██████████| 4005/4005 [00:00<00:00, 71467.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/gaz_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for som_Latn (sm)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1235959/1235959 [00:16<00:00, 73959.48 examples/s]\n",
      "Generating eval split: 100%|██████████| 137938/137938 [00:02<00:00, 68609.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/som_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for sna_Latn (sn)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 141559/141559 [00:02<00:00, 54297.09 examples/s]\n",
      "Generating eval split: 100%|██████████| 16126/16126 [00:00<00:00, 38758.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/sna_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for sot_Latn (st)...\n",
      "Saved to ../../scratch/data/data_pretrain/wura/sot_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for swc_Latn (sw)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1801101/1801101 [00:33<00:00, 54058.79 examples/s] \n",
      "Generating eval split: 100%|██████████| 200345/200345 [00:03<00:00, 55826.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/swc_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for tir_Ethi (ti)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9807/9807 [00:00<00:00, 28713.63 examples/s]\n",
      "Generating eval split: 100%|██████████| 1084/1084 [00:00<00:00, 43425.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/tir_Ethi_wura.jsonl\n",
      "⬇ Downloading WURA for xho_Latn (xh)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 69713/69713 [00:01<00:00, 61956.25 examples/s]\n",
      "Generating eval split: 100%|██████████| 7846/7846 [00:00<00:00, 73477.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/xho_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for yor_Latn (yo)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 141321/141321 [00:02<00:00, 68143.40 examples/s] \n",
      "Generating eval split: 100%|██████████| 15612/15612 [00:00<00:00, 71896.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/yor_Latn_wura.jsonl\n",
      "⬇ Downloading WURA for zul_Latn (zu)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 166370/166370 [00:03<00:00, 47804.97 examples/s]\n",
      "Generating eval split: 100%|██████████| 18289/18289 [00:00<00:00, 56318.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../scratch/data/data_pretrain/wura/zul_Latn_wura.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "output_dir = \"../../scratch/data/data_pretrain/wura\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each language\n",
    "for long_code, wura_code in wura_langs.items():\n",
    "    output_path = os.path.join(output_dir, f\"{long_code}_wura.jsonl\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{long_code}: {output_path} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"⬇ Downloading WURA for {long_code} ({wura_code})...\")\n",
    "\n",
    "    try:\n",
    "        ds = load_dataset(\"llama-lang-adapt/wura\", name=wura_code, split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {wura_code}: {e}\")\n",
    "        continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in ds:\n",
    "            # Check if there's a text field in the example\n",
    "            if \"text\" in example:\n",
    "                json.dump({\"text\": example[\"text\"]}, f)\n",
    "            else:\n",
    "                # If not, you might need to identify the relevant field or concatenate multiple fields\n",
    "                # This is a fallback if you're not sure about the structure\n",
    "                text = str(example)\n",
    "                json.dump({\"text\": text}, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3629992",
   "metadata": {},
   "source": [
    "# Madlad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ../../scratch/data/data_pretrain/madlad400/afr_Latn_madlad.jsonl (streamed 868671 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/aka_Latn_madlad.jsonl (streamed 4768 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/amh_Ethi_madlad.jsonl (streamed 106301 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/bam_Latn_madlad.jsonl (streamed 702 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/dik_Latn_madlad.jsonl (streamed 611 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/dyu_Latn_madlad.jsonl (streamed 483 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/ewe_Latn_madlad.jsonl (streamed 4536 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/fon_Latn_madlad.jsonl (streamed 1065 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/fuv_Latn_madlad.jsonl (streamed 26 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/gaz_Latn_madlad.jsonl (streamed 18895 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/hau_Latn_madlad.jsonl (streamed 173485 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/ibo_Latn_madlad.jsonl (streamed 54410 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/kbp_Latn_madlad.jsonl (streamed 3036 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/kin_Latn_madlad.jsonl (streamed 226466 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/kmb_Latn_madlad.jsonl (streamed 538 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/kon_Latn_madlad.jsonl (streamed 365 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/lin_Latn_madlad.jsonl (streamed 3325 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/lug_Latn_madlad.jsonl (streamed 13030 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/run_Latn_madlad.jsonl (streamed 323 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/sag_Latn_madlad.jsonl (streamed 2106 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/sna_Latn_madlad.jsonl (streamed 60196 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/som_Latn_madlad.jsonl (streamed 293218 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/sot_Latn_madlad.jsonl (streamed 40360 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/ssw_Latn_madlad.jsonl (streamed 1089 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/swc_Latn_madlad.jsonl (streamed 537847 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/tir_Ethi_madlad.jsonl (streamed 7288 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/tsn_Latn_madlad.jsonl (streamed 4821 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/tso_Latn_madlad.jsonl (streamed 5198 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/tzm_Tfng_madlad.jsonl (streamed 79 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/wol_Latn_madlad.jsonl (streamed 871 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/xho_Latn_madlad.jsonl (streamed 53672 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/yor_Latn_madlad.jsonl (streamed 52067 docs)\n",
      "Done: ../../scratch/data/data_pretrain/madlad400/zul_Latn_madlad.jsonl (streamed 53809 docs)\n"
     ]
    }
   ],
   "source": [
    "# Folder\n",
    "output_dir = \"../../scratch/data/data_pretrain/madlad400\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# write every 1000 docs\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "for fineweb_code, madlad_code in madlad_langs.items():\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{fineweb_code}_madlad.jsonl\")\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{fineweb_code}: {output_path} already exists, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"allenai/madlad-400\", languages=[madlad_code], split=\"clean\", streaming=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to stream {madlad_code}: {e}\")\n",
    "        continue\n",
    "\n",
    "    buffer = []\n",
    "    i = 0\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, example in enumerate(dataset, 1):\n",
    "            # Extract the text field from the MADLAD example\n",
    "            if 'text' in example:\n",
    "                # Create the standardized format with \"text\" field\n",
    "                formatted_example = {\"text\": example['text']}\n",
    "                buffer.append(json.dumps(formatted_example))\n",
    "            else:\n",
    "                print(f\"Warning: No 'text' field found in example {i} for {madlad_code}\")\n",
    "                continue\n",
    "\n",
    "            # write buffer to file every BUFFER_SIZE items\n",
    "            if len(buffer) >= BUFFER_SIZE:\n",
    "                f.write(\"\\n\".join(buffer) + \"\\n\")\n",
    "                buffer = []\n",
    "\n",
    "        # write remaining items\n",
    "        if buffer:\n",
    "            f.write(\"\\n\".join(buffer) + \"\\n\")\n",
    "\n",
    "    print(f\"Done: {output_path} (streamed {i} docs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608d8eb",
   "metadata": {},
   "source": [
    "# Other Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9edc72",
   "metadata": {},
   "source": [
    "## Malagsay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93af6990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marothodi example structure: {'text': 'Ka goo Ruthe aya le Naomi Betlehema nageng ya Israele.', 'source': 'https://downloads.wortschatz-leipzig.de/corpora/tsn_community_2017.tar.gz', 'source-category': 'tsn_community_2017'}\n",
      "../../scratch/data/data_pretrain/extradata/tsn_Latn.jsonl already contains all 152464 examples, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Output folder & path\n",
    "output_dir = \"../../scratch/data/data_pretrain/extradata\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"tsn_Latn.jsonl\")\n",
    "\n",
    "# Load the extra dataset\n",
    "ds = load_dataset(\"OxxoCodes/Marothodi\", split=\"train\")\n",
    "\n",
    "# Print structure of first example to verify\n",
    "if len(ds) > 0:\n",
    "    print(\"Marothodi example structure:\", ds[0])\n",
    "\n",
    "# Check if the file exists and how many lines it has\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_lines = sum(1 for _ in f)\n",
    "\n",
    "    if existing_lines >= len(ds):\n",
    "        print(f\"{output_path} already contains all {existing_lines} examples, skipping.\")\n",
    "    else:\n",
    "        print(f\"{output_path} has {existing_lines}/{len(ds)} examples — appending missing...\")\n",
    "        with open(output_path, \"a\", encoding=\"utf-8\") as out_f:\n",
    "            for i, example in enumerate(ds):\n",
    "                if i < existing_lines:\n",
    "                    continue\n",
    "                \n",
    "                # Create standardized format with \"text\" field\n",
    "                if \"text\" in example:\n",
    "                    json.dump({\"text\": example[\"text\"]}, out_f)\n",
    "                elif \"sentence\" in example:  # Assuming the dataset might have a \"sentence\" field\n",
    "                    json.dump({\"text\": example[\"sentence\"]}, out_f)\n",
    "                else:\n",
    "                    # If no clear text field, identify the appropriate field based on dataset structure\n",
    "                    # For example, concatenate multiple fields or use a specific field:\n",
    "                    # You may need to adjust this based on the actual structure\n",
    "                    content = str(example)\n",
    "                    json.dump({\"text\": content}, out_f)\n",
    "                \n",
    "                out_f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"Appended {len(ds) - existing_lines} new examples to {output_path}\")\n",
    "else:\n",
    "    # File does not exist; write all from scratch\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for example in ds:\n",
    "            # Create standardized format with \"text\" field\n",
    "            if \"text\" in example:\n",
    "                json.dump({\"text\": example[\"text\"]}, out_f)\n",
    "            elif \"sentence\" in example:  # Assuming the dataset might have a \"sentence\" field\n",
    "                json.dump({\"text\": example[\"sentence\"]}, out_f)\n",
    "            else:\n",
    "                # If no clear text field, identify the appropriate field based on dataset structure\n",
    "                content = str(example)\n",
    "                json.dump({\"text\": content}, out_f)\n",
    "            \n",
    "            out_f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Wrote all {len(ds)} examples to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dafa4a",
   "metadata": {},
   "source": [
    "# Combine everything and LlaMa Factory Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5afbda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating datasets...\n",
      "\n",
      "Processing fineweb2 datasets...\n",
      "  Reading aeb_Arab_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 262,884 examples from aeb_Arab_fw2.jsonl\n",
      "  Reading afr_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Added 877,109 examples from afr_Latn_fw2.jsonl\n",
      "  Reading amh_Ethi_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 280,355 examples from amh_Ethi_fw2.jsonl\n",
      "  Reading arz_Arab_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Processed 1,400,000 lines...\n",
      "    Added 1,410,134 examples from arz_Arab_fw2.jsonl\n",
      "  Reading bam_Latn_fw2.jsonl...\n",
      "    Added 14,044 examples from bam_Latn_fw2.jsonl\n",
      "  Reading bem_Latn_fw2.jsonl...\n",
      "    Added 1,143 examples from bem_Latn_fw2.jsonl\n",
      "  Reading cjk_Latn_fw2.jsonl...\n",
      "    Added 44 examples from cjk_Latn_fw2.jsonl\n",
      "  Reading dyu_Latn_fw2.jsonl...\n",
      "    Added 2,209 examples from dyu_Latn_fw2.jsonl\n",
      "  Reading gaz_Latn_fw2.jsonl...\n",
      "    Added 43,468 examples from gaz_Latn_fw2.jsonl\n",
      "  Reading ibo_Latn_fw2.jsonl...\n",
      "    Added 95,184 examples from ibo_Latn_fw2.jsonl\n",
      "  Reading kab_Latn_fw2.jsonl...\n",
      "    Added 7,717 examples from kab_Latn_fw2.jsonl\n",
      "  Reading kam_Latn_fw2.jsonl...\n",
      "    Added 1,218 examples from kam_Latn_fw2.jsonl\n",
      "  Reading kbp_Latn_fw2.jsonl...\n",
      "    Added 1,231 examples from kbp_Latn_fw2.jsonl\n",
      "  Reading kin_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 199,112 examples from kin_Latn_fw2.jsonl\n",
      "  Reading kmb_Latn_fw2.jsonl...\n",
      "    Added 1,132 examples from kmb_Latn_fw2.jsonl\n",
      "  Reading knc_Arab_fw2.jsonl...\n",
      "    Added 290 examples from knc_Arab_fw2.jsonl\n",
      "  Reading knc_Latn_fw2.jsonl...\n",
      "    Added 437 examples from knc_Latn_fw2.jsonl\n",
      "  Reading lin_Latn_fw2.jsonl...\n",
      "    Added 15,241 examples from lin_Latn_fw2.jsonl\n",
      "  Reading lug_Latn_fw2.jsonl...\n",
      "    Added 32,954 examples from lug_Latn_fw2.jsonl\n",
      "  Reading luo_Latn_fw2.jsonl...\n",
      "    Added 2,210 examples from luo_Latn_fw2.jsonl\n",
      "  Reading nus_Latn_fw2.jsonl...\n",
      "    Added 152 examples from nus_Latn_fw2.jsonl\n",
      "  Reading plt_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 254,482 examples from plt_Latn_fw2.jsonl\n",
      "  Reading run_Latn_fw2.jsonl...\n",
      "    Added 88,823 examples from run_Latn_fw2.jsonl\n",
      "  Reading sag_Latn_fw2.jsonl...\n",
      "    Added 4,537 examples from sag_Latn_fw2.jsonl\n",
      "  Reading sna_Latn_fw2.jsonl...\n",
      "    Added 80,003 examples from sna_Latn_fw2.jsonl\n",
      "  Reading sot_Latn_fw2.jsonl...\n",
      "    Added 83,329 examples from sot_Latn_fw2.jsonl\n",
      "  Reading ssw_Latn_fw2.jsonl...\n",
      "    Added 1,668 examples from ssw_Latn_fw2.jsonl\n",
      "  Reading swc_Latn_fw2.jsonl...\n",
      "    Added 2,161 examples from swc_Latn_fw2.jsonl\n",
      "  Reading taq_Tfng_fw2.jsonl...\n",
      "    Added 208 examples from taq_Tfng_fw2.jsonl\n",
      "  Reading tir_Ethi_fw2.jsonl...\n",
      "    Added 65,569 examples from tir_Ethi_fw2.jsonl\n",
      "  Reading tsn_Latn_fw2.jsonl...\n",
      "    Added 5,530 examples from tsn_Latn_fw2.jsonl\n",
      "  Reading twi_Latn_fw2.jsonl...\n",
      "    Added 5,655 examples from twi_Latn_fw2.jsonl\n",
      "  Reading tzm_Tfng_fw2.jsonl...\n",
      "    Added 2,376 examples from tzm_Tfng_fw2.jsonl\n",
      "  Reading umb_Latn_fw2.jsonl...\n",
      "    Added 709 examples from umb_Latn_fw2.jsonl\n",
      "  Reading xho_Latn_fw2.jsonl...\n",
      "    Added 99,567 examples from xho_Latn_fw2.jsonl\n",
      "  Reading yor_Latn_fw2.jsonl...\n",
      "    Added 67,447 examples from yor_Latn_fw2.jsonl\n",
      "  Reading zul_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 116,693 examples from zul_Latn_fw2.jsonl\n",
      "  Reading spa_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Added 1,375,812 examples from spa_Latn_fw2.jsonl\n",
      "  Reading fra_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Added 1,286,809 examples from fra_Latn_fw2.jsonl\n",
      "  Reading por_Latn_fw2.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Added 1,375,915 examples from por_Latn_fw2.jsonl\n",
      "\n",
      "Processing afrimgsm datasets...\n",
      "  Reading amh_Ethi_mgsm.jsonl...\n",
      "    Added 8 examples from amh_Ethi_mgsm.jsonl\n",
      "  Reading ewe_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from ewe_Latn_mgsm.jsonl\n",
      "  Reading gaz_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from gaz_Latn_mgsm.jsonl\n",
      "  Reading hau_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from hau_Latn_mgsm.jsonl\n",
      "  Reading kin_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from kin_Latn_mgsm.jsonl\n",
      "  Reading lin_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from lin_Latn_mgsm.jsonl\n",
      "  Reading lug_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from lug_Latn_mgsm.jsonl\n",
      "  Reading sna_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from sna_Latn_mgsm.jsonl\n",
      "  Reading swc_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from swc_Latn_mgsm.jsonl\n",
      "  Reading twi_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from twi_Latn_mgsm.jsonl\n",
      "  Reading wol_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from wol_Latn_mgsm.jsonl\n",
      "  Reading xho_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from xho_Latn_mgsm.jsonl\n",
      "  Reading yor_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from yor_Latn_mgsm.jsonl\n",
      "  Reading zul_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from zul_Latn_mgsm.jsonl\n",
      "\n",
      "Processing mgsm datasets...\n",
      "  Reading eng_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from eng_Latn_mgsm.jsonl\n",
      "  Reading spa_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from spa_Latn_mgsm.jsonl\n",
      "  Reading fra_Latn_mgsm.jsonl...\n",
      "    Added 8 examples from fra_Latn_mgsm.jsonl\n",
      "\n",
      "Processing madlad400 datasets...\n",
      "  Reading afr_Latn_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Added 868,671 examples from afr_Latn_madlad.jsonl\n",
      "  Reading aka_Latn_madlad.jsonl...\n",
      "    Added 4,768 examples from aka_Latn_madlad.jsonl\n",
      "  Reading amh_Ethi_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 106,301 examples from amh_Ethi_madlad.jsonl\n",
      "  Reading bam_Latn_madlad.jsonl...\n",
      "    Added 702 examples from bam_Latn_madlad.jsonl\n",
      "  Reading dik_Latn_madlad.jsonl...\n",
      "    Added 611 examples from dik_Latn_madlad.jsonl\n",
      "  Reading dyu_Latn_madlad.jsonl...\n",
      "    Added 483 examples from dyu_Latn_madlad.jsonl\n",
      "  Reading ewe_Latn_madlad.jsonl...\n",
      "    Added 4,536 examples from ewe_Latn_madlad.jsonl\n",
      "  Reading fon_Latn_madlad.jsonl...\n",
      "    Added 1,065 examples from fon_Latn_madlad.jsonl\n",
      "  Reading fuv_Latn_madlad.jsonl...\n",
      "    Added 26 examples from fuv_Latn_madlad.jsonl\n",
      "  Reading gaz_Latn_madlad.jsonl...\n",
      "    Added 18,895 examples from gaz_Latn_madlad.jsonl\n",
      "  Reading hau_Latn_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 173,485 examples from hau_Latn_madlad.jsonl\n",
      "  Reading ibo_Latn_madlad.jsonl...\n",
      "    Added 54,410 examples from ibo_Latn_madlad.jsonl\n",
      "  Reading kbp_Latn_madlad.jsonl...\n",
      "    Added 3,036 examples from kbp_Latn_madlad.jsonl\n",
      "  Reading kin_Latn_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 226,466 examples from kin_Latn_madlad.jsonl\n",
      "  Reading kmb_Latn_madlad.jsonl...\n",
      "    Added 538 examples from kmb_Latn_madlad.jsonl\n",
      "  Reading kon_Latn_madlad.jsonl...\n",
      "    Added 365 examples from kon_Latn_madlad.jsonl\n",
      "  Reading lin_Latn_madlad.jsonl...\n",
      "    Added 3,325 examples from lin_Latn_madlad.jsonl\n",
      "  Reading lug_Latn_madlad.jsonl...\n",
      "    Added 13,030 examples from lug_Latn_madlad.jsonl\n",
      "  Reading run_Latn_madlad.jsonl...\n",
      "    Added 323 examples from run_Latn_madlad.jsonl\n",
      "  Reading sag_Latn_madlad.jsonl...\n",
      "    Added 2,106 examples from sag_Latn_madlad.jsonl\n",
      "  Reading sna_Latn_madlad.jsonl...\n",
      "    Added 60,196 examples from sna_Latn_madlad.jsonl\n",
      "  Reading som_Latn_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 293,218 examples from som_Latn_madlad.jsonl\n",
      "  Reading sot_Latn_madlad.jsonl...\n",
      "    Added 40,360 examples from sot_Latn_madlad.jsonl\n",
      "  Reading ssw_Latn_madlad.jsonl...\n",
      "    Added 1,089 examples from ssw_Latn_madlad.jsonl\n",
      "  Reading swc_Latn_madlad.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Added 537,847 examples from swc_Latn_madlad.jsonl\n",
      "  Reading tir_Ethi_madlad.jsonl...\n",
      "    Added 7,288 examples from tir_Ethi_madlad.jsonl\n",
      "  Reading tsn_Latn_madlad.jsonl...\n",
      "    Added 4,821 examples from tsn_Latn_madlad.jsonl\n",
      "  Reading tso_Latn_madlad.jsonl...\n",
      "    Added 5,198 examples from tso_Latn_madlad.jsonl\n",
      "  Reading tzm_Tfng_madlad.jsonl...\n",
      "    Added 79 examples from tzm_Tfng_madlad.jsonl\n",
      "  Reading wol_Latn_madlad.jsonl...\n",
      "    Added 871 examples from wol_Latn_madlad.jsonl\n",
      "  Reading xho_Latn_madlad.jsonl...\n",
      "    Added 53,672 examples from xho_Latn_madlad.jsonl\n",
      "  Reading yor_Latn_madlad.jsonl...\n",
      "    Added 52,067 examples from yor_Latn_madlad.jsonl\n",
      "  Reading zul_Latn_madlad.jsonl...\n",
      "    Added 53,809 examples from zul_Latn_madlad.jsonl\n",
      "\n",
      "Processing wura datasets...\n",
      "  Reading afr_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Processed 1,400,000 lines...\n",
      "    Processed 1,500,000 lines...\n",
      "    Processed 1,600,000 lines...\n",
      "    Processed 1,700,000 lines...\n",
      "    Processed 1,800,000 lines...\n",
      "    Processed 1,900,000 lines...\n",
      "    Processed 2,000,000 lines...\n",
      "    Processed 2,100,000 lines...\n",
      "    Processed 2,200,000 lines...\n",
      "    Processed 2,300,000 lines...\n",
      "    Added 2,390,884 examples from afr_Latn_wura.jsonl\n",
      "  Reading amh_Ethi_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Added 291,026 examples from amh_Ethi_wura.jsonl\n",
      "  Reading arz_Arab_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Added 1,116,034 examples from arz_Arab_wura.jsonl\n",
      "  Reading hau_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Added 565,471 examples from hau_Latn_wura.jsonl\n",
      "  Reading ibo_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 121,421 examples from ibo_Latn_wura.jsonl\n",
      "  Reading kin_Latn_wura.jsonl...\n",
      "    Added 61,485 examples from kin_Latn_wura.jsonl\n",
      "  Reading plt_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Added 355,390 examples from plt_Latn_wura.jsonl\n",
      "  Reading gaz_Latn_wura.jsonl...\n",
      "    Added 37,280 examples from gaz_Latn_wura.jsonl\n",
      "  Reading som_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Added 1,235,959 examples from som_Latn_wura.jsonl\n",
      "  Reading sna_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 141,559 examples from sna_Latn_wura.jsonl\n",
      "  Reading sot_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 124,082 examples from sot_Latn_wura.jsonl\n",
      "  Reading swc_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Processed 1,400,000 lines...\n",
      "    Processed 1,500,000 lines...\n",
      "    Processed 1,600,000 lines...\n",
      "    Processed 1,700,000 lines...\n",
      "    Processed 1,800,000 lines...\n",
      "    Added 1,801,101 examples from swc_Latn_wura.jsonl\n",
      "  Reading tir_Ethi_wura.jsonl...\n",
      "    Added 9,807 examples from tir_Ethi_wura.jsonl\n",
      "  Reading xho_Latn_wura.jsonl...\n",
      "    Added 69,713 examples from xho_Latn_wura.jsonl\n",
      "  Reading yor_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 141,321 examples from yor_Latn_wura.jsonl\n",
      "  Reading zul_Latn_wura.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 166,370 examples from zul_Latn_wura.jsonl\n",
      "\n",
      "Processing extradata datasets...\n",
      "  Reading tsn_Latn.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Added 152,464 examples from tsn_Latn.jsonl\n",
      "\n",
      "Processing fineweb datasets...\n",
      "  Reading eng_Latn.jsonl...\n",
      "    Processed 100,000 lines...\n",
      "    Processed 200,000 lines...\n",
      "    Processed 300,000 lines...\n",
      "    Processed 400,000 lines...\n",
      "    Processed 500,000 lines...\n",
      "    Processed 600,000 lines...\n",
      "    Processed 700,000 lines...\n",
      "    Processed 800,000 lines...\n",
      "    Processed 900,000 lines...\n",
      "    Processed 1,000,000 lines...\n",
      "    Processed 1,100,000 lines...\n",
      "    Processed 1,200,000 lines...\n",
      "    Processed 1,300,000 lines...\n",
      "    Processed 1,400,000 lines...\n",
      "    Added 1,465,680 examples from eng_Latn.jsonl\n",
      "\n",
      "Creating dataset_info.json...\n",
      "\n",
      "=== Summary ===\n",
      "Total languages: 51\n",
      "Total examples: 21,006,401\n",
      "  afr_Latn: 4,136,664 examples\n",
      "  arz_Arab: 2,526,168 examples\n",
      "  swc_Latn: 2,341,117 examples\n",
      "  som_Latn: 1,529,177 examples\n",
      "  eng_Latn: 1,465,688 examples\n",
      "  por_Latn: 1,375,915 examples\n",
      "  spa_Latn: 1,375,820 examples\n",
      "  fra_Latn: 1,286,817 examples\n",
      "  hau_Latn: 738,964 examples\n",
      "  amh_Ethi: 677,690 examples\n",
      "  plt_Latn: 609,872 examples\n",
      "  kin_Latn: 487,071 examples\n",
      "  zul_Latn: 336,880 examples\n",
      "  sna_Latn: 281,766 examples\n",
      "  ibo_Latn: 271,015 examples\n",
      "  aeb_Arab: 262,884 examples\n",
      "  yor_Latn: 260,843 examples\n",
      "  sot_Latn: 247,771 examples\n",
      "  xho_Latn: 222,960 examples\n",
      "  tsn_Latn: 162,815 examples\n",
      "  gaz_Latn: 99,651 examples\n",
      "  run_Latn: 89,146 examples\n",
      "  tir_Ethi: 82,664 examples\n",
      "  lug_Latn: 45,992 examples\n",
      "  lin_Latn: 18,574 examples\n",
      "  bam_Latn: 14,746 examples\n",
      "  kab_Latn: 7,717 examples\n",
      "  sag_Latn: 6,643 examples\n",
      "  twi_Latn: 5,663 examples\n",
      "  tso_Latn: 5,198 examples\n",
      "  aka_Latn: 4,768 examples\n",
      "  ewe_Latn: 4,544 examples\n",
      "  kbp_Latn: 4,267 examples\n",
      "  ssw_Latn: 2,757 examples\n",
      "  dyu_Latn: 2,692 examples\n",
      "  tzm_Tfng: 2,455 examples\n",
      "  luo_Latn: 2,210 examples\n",
      "  kmb_Latn: 1,670 examples\n",
      "  kam_Latn: 1,218 examples\n",
      "  bem_Latn: 1,143 examples\n",
      "  fon_Latn: 1,065 examples\n",
      "  wol_Latn: 879 examples\n",
      "  umb_Latn: 709 examples\n",
      "  dik_Latn: 611 examples\n",
      "  knc_Latn: 437 examples\n",
      "  kon_Latn: 365 examples\n",
      "  knc_Arab: 290 examples\n",
      "  taq_Tfng: 208 examples\n",
      "  nus_Latn: 152 examples\n",
      "  cjk_Latn: 44 examples\n",
      "  fuv_Latn: 26 examples\n",
      "\n",
      "All datasets saved to /home/mila/x/xut/scratch/data/data_pretrain/llama_factory_data\n",
      "Dataset info saved to /home/mila/x/xut/scratch/data/data_pretrain/llama_factory_data/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc  # Garbage collector\n",
    "\n",
    "# Set up paths\n",
    "base_dir = \"/home/mila/x/xut/scratch/data/data_pretrain\"\n",
    "final_output_folder = os.path.join(base_dir, \"llama_factory_data\")  \n",
    "os.makedirs(final_output_folder, exist_ok=True)\n",
    "\n",
    "# Dataset folders & suffixes\n",
    "datasets = [\n",
    "    (\"fineweb2\",   \"_fw2.jsonl\"),\n",
    "    (\"afrimgsm\",   \"_mgsm.jsonl\"),\n",
    "    (\"mgsm\",       \"_mgsm.jsonl\"),\n",
    "    (\"madlad400\",  \"_madlad.jsonl\"),\n",
    "    (\"wura\",       \"_wura.jsonl\"),\n",
    "    (\"extradata\",  \".jsonl\"),\n",
    "    (\"fineweb\",    \".jsonl\")\n",
    "]\n",
    "\n",
    "# Track statistics without keeping all data in memory\n",
    "dataset_stats = {}\n",
    "language_counts = {}\n",
    "\n",
    "# Create output files for each language up front\n",
    "language_files = {}\n",
    "\n",
    "print(\"Consolidating datasets...\")\n",
    "\n",
    "# Process each dataset folder\n",
    "for folder_name, suffix in datasets:\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} doesn't exist, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing {folder_name} datasets...\")\n",
    "    \n",
    "    # Find all JSONL files in the folder\n",
    "    jsonl_pattern = os.path.join(folder_path, f\"*{suffix}\")\n",
    "    jsonl_files = glob.glob(jsonl_pattern)\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"  No files matching pattern: {jsonl_pattern}\")\n",
    "        continue\n",
    "    \n",
    "    for file_path in jsonl_files:\n",
    "        try:\n",
    "            # Extract language code from filename\n",
    "            filename = os.path.basename(file_path)\n",
    "            lang_code = filename.replace(suffix, \"\")\n",
    "            \n",
    "            print(f\"  Reading {filename}...\")\n",
    "            \n",
    "            # Initialize count for this language if not already done\n",
    "            if lang_code not in language_counts:\n",
    "                language_counts[lang_code] = 0\n",
    "            \n",
    "            # Initialize the output file if not already done\n",
    "            if lang_code not in language_files:\n",
    "                output_path = os.path.join(final_output_folder, f\"{lang_code}_data.jsonl\")\n",
    "                language_files[lang_code] = output_path\n",
    "            \n",
    "            # Process line by line to avoid memory issues\n",
    "            examples_processed = 0\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as in_file, \\\n",
    "                 open(language_files[lang_code], 'a', encoding='utf-8') as out_file:\n",
    "                \n",
    "                for line_num, line in enumerate(in_file, 1):\n",
    "                    try:\n",
    "                        # Skip empty lines\n",
    "                        if not line.strip():\n",
    "                            continue\n",
    "                        \n",
    "                        item = json.loads(line.strip())\n",
    "                        \n",
    "                        # Ensure we have the {\"text\": content} format\n",
    "                        if 'text' in item and item['text']:\n",
    "                            # Write directly to output file, don't store in memory\n",
    "                            out_file.write(json.dumps({\"text\": item['text']}, ensure_ascii=False) + '\\n')\n",
    "                            examples_processed += 1\n",
    "                        else:\n",
    "                            # If no text field, try to find the right field\n",
    "                            if isinstance(item, dict):\n",
    "                                # Find the first value that looks like text\n",
    "                                for key, value in item.items():\n",
    "                                    if isinstance(value, str) and len(value) > 10:\n",
    "                                        out_file.write(json.dumps({\"text\": value}, ensure_ascii=False) + '\\n')\n",
    "                                        examples_processed += 1\n",
    "                                        break\n",
    "                            \n",
    "                        # Periodically report progress and clear memory\n",
    "                        if line_num % 100000 == 0:\n",
    "                            print(f\"    Processed {line_num:,} lines...\")\n",
    "                            gc.collect()  # Force garbage collection\n",
    "                    \n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"    Warning: Skipping invalid JSON at line {line_num} in {filename}\")\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error processing line {line_num} in {filename}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Update the count for this language\n",
    "            language_counts[lang_code] += examples_processed\n",
    "            print(f\"    Added {examples_processed:,} examples from {filename}\")\n",
    "            \n",
    "            # Force garbage collection after each file\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create dataset info JSON for LLaMA Factory\n",
    "print(\"\\nCreating dataset_info.json...\")\n",
    "dataset_info = {}\n",
    "\n",
    "for lang_code, count in language_counts.items():\n",
    "    # Add entry to dataset info (LLaMA Factory format)\n",
    "    dataset_info[f\"{lang_code}_dataset\"] = {\n",
    "        \"file_name\": f\"{lang_code}_data.jsonl\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"text\"  # Maps the \"text\" field to the \"prompt\" column for LLaMA Factory\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Save dataset info\n",
    "dataset_info_path = os.path.join(final_output_folder, \"dataset_info.json\")\n",
    "with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total languages: {len(language_counts)}\")\n",
    "total_examples = sum(language_counts.values())\n",
    "print(f\"Total examples: {total_examples:,}\")\n",
    "for lang, count in sorted(language_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {lang}: {count:,} examples\")\n",
    "print(f\"\\nAll datasets saved to {final_output_folder}\")\n",
    "print(f\"Dataset info saved to {dataset_info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50df0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_language_list = [\n",
    "    'aeb_Arab',\n",
    "    'afr_Latn',\n",
    "    'aka_Latn',\n",
    "    'amh_Ethi',\n",
    "    'ary_Arab',\n",
    "    'arz_Arab',\n",
    "    'bam_Latn',\n",
    "    'bem_Latn',\n",
    "    'cjk_Latn',\n",
    "    'dik_Latn',\n",
    "    'dyu_Latn',\n",
    "    'ewe_Latn',\n",
    "    'fon_Latn',\n",
    "    'fuv_Latn',\n",
    "    'gaz_Latn',\n",
    "    'hau_Latn',\n",
    "    'ibo_Latn',\n",
    "    'kab_Latn',\n",
    "    'kam_Latn',\n",
    "    'kbp_Latn',\n",
    "    'kea_Latn',\n",
    "    'kik_Tatn',\n",
    "    'kin_Latn',\n",
    "    'kmb_Latn',\n",
    "    'knc_Arab',\n",
    "    'knc_Latn',\n",
    "    'kon_Latn',\n",
    "    'lin_Latn',\n",
    "    'lua_Latn',\n",
    "    'lug_Latn',\n",
    "    'luo_Latn',\n",
    "    'Mos_Latn',\n",
    "    'nqo_Nkoo',\n",
    "    'nso_Latn',\n",
    "    'nus_Latn',\n",
    "    'nya_Latn',\n",
    "    'plt_Latn',\n",
    "    'run_Latn',\n",
    "    'sag_Latn',\n",
    "    'sna_Latn',\n",
    "    'som_Latn',\n",
    "    'sot_Latn',\n",
    "    'ssw_Latn',\n",
    "    'swc_Latn',\n",
    "    'taq_Latn',\n",
    "    'taq_Tfng',\n",
    "    'tir_Ethi',\n",
    "    'tsn_Latn',\n",
    "    'tso_Latn',\n",
    "    'tum_Latn',\n",
    "    'twi_Latn',\n",
    "    'tzm_Tfng',\n",
    "    'umb_Latn',\n",
    "    'wol_Latn',\n",
    "    'xho_Latn',\n",
    "    'yor_Latn',\n",
    "    'zul_Latn',\n",
    "    'eng_Latn',\n",
    "    'spa_Latn',\n",
    "    'fra_Latn',\n",
    "    'por_Latn'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66436f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ aeb_Arab_data.jsonl → 594.40 MB (623273088 B) → gzip 147.30 MB → 262884 examples\n",
      "✓ afr_Latn_data.jsonl → 11.64 GB (12496774779 B) → gzip 4.45 GB → 4136664 examples\n",
      "✓ aka_Latn_data.jsonl → 25.76 MB (27014100 B) → gzip 9.01 MB → 4768 examples\n",
      "✓ amh_Ethi_data.jsonl → 3.75 GB (4027695752 B) → gzip 1.04 GB → 677690 examples\n",
      "✓ arz_Arab_data.jsonl → 3.88 GB (4165053531 B) → gzip 1.09 GB → 2526168 examples\n",
      "✓ bam_Latn_data.jsonl → 29.93 MB (31383088 B) → gzip 7.06 MB → 14746 examples\n",
      "✓ bem_Latn_data.jsonl → 3.97 MB (4157790 B) → gzip 1.37 MB → 1143 examples\n",
      "✓ cjk_Latn_data.jsonl → 81.79 KB (83756 B) → gzip 24.13 KB → 44 examples\n",
      "✓ dik_Latn_data.jsonl → 3.17 MB (3328890 B) → gzip 1.07 MB → 611 examples\n",
      "✓ dyu_Latn_data.jsonl → 6.53 MB (6845028 B) → gzip 2.00 MB → 2692 examples\n",
      "✓ eng_Latn_data.jsonl → 4.23 GB (4547144294 B) → gzip 1.67 GB → 1465688 examples\n",
      "✓ ewe_Latn_data.jsonl → 34.98 MB (36682212 B) → gzip 11.73 MB → 4544 examples\n",
      "✓ fon_Latn_data.jsonl → 9.79 MB (10262740 B) → gzip 3.07 MB → 1065 examples\n",
      "✓ fra_Latn_data.jsonl → 3.96 GB (4248475688 B) → gzip 1.50 GB → 1286817 examples\n",
      "✓ fuv_Latn_data.jsonl → 273.84 KB (280409 B) → gzip 78.60 KB → 26 examples\n",
      "✓ gaz_Latn_data.jsonl → 243.53 MB (255363791 B) → gzip 84.53 MB → 99651 examples\n",
      "✓ hau_Latn_data.jsonl → 1.23 GB (1318350635 B) → gzip 421.80 MB → 738964 examples\n",
      "✓ ibo_Latn_data.jsonl → 830.76 MB (871117397 B) → gzip 263.75 MB → 271015 examples\n",
      "✓ kab_Latn_data.jsonl → 9.22 MB (9666129 B) → gzip 3.96 MB → 7717 examples\n",
      "✓ kam_Latn_data.jsonl → 2.34 MB (2455460 B) → gzip 774.22 KB → 1218 examples\n",
      "✓ kbp_Latn_data.jsonl → 19.61 MB (20558356 B) → gzip 6.32 MB → 4267 examples\n",
      "✓ kin_Latn_data.jsonl → 1.15 GB (1232948697 B) → gzip 422.74 MB → 487071 examples\n",
      "✓ kmb_Latn_data.jsonl → 8.36 MB (8768336 B) → gzip 2.55 MB → 1670 examples\n",
      "✓ knc_Arab_data.jsonl → 9.88 MB (10357031 B) → gzip 2.63 MB → 290 examples\n",
      "✓ knc_Latn_data.jsonl → 810.73 KB (830183 B) → gzip 206.96 KB → 437 examples\n",
      "✓ kon_Latn_data.jsonl → 2.54 MB (2664011 B) → gzip 903.60 KB → 365 examples\n",
      "✓ lin_Latn_data.jsonl → 70.13 MB (73538570 B) → gzip 24.14 MB → 18574 examples\n",
      "✓ lug_Latn_data.jsonl → 111.75 MB (117176365 B) → gzip 41.82 MB → 45992 examples\n",
      "✓ luo_Latn_data.jsonl → 5.58 MB (5849981 B) → gzip 2.00 MB → 2210 examples\n",
      "✓ nus_Latn_data.jsonl → 505.00 KB (517122 B) → gzip 163.15 KB → 152 examples\n",
      "✓ plt_Latn_data.jsonl → 1.29 GB (1389187940 B) → gzip 450.15 MB → 609872 examples\n",
      "✓ por_Latn_data.jsonl → 4.02 GB (4315359115 B) → gzip 1.54 GB → 1375915 examples\n",
      "✓ run_Latn_data.jsonl → 155.33 MB (162878486 B) → gzip 57.46 MB → 89146 examples\n",
      "✓ sag_Latn_data.jsonl → 34.68 MB (36360438 B) → gzip 10.12 MB → 6643 examples\n",
      "✓ sna_Latn_data.jsonl → 725.59 MB (760833803 B) → gzip 262.35 MB → 281766 examples\n",
      "✓ som_Latn_data.jsonl → 2.21 GB (2368922472 B) → gzip 814.97 MB → 1529177 examples\n",
      "✓ sot_Latn_data.jsonl → 742.15 MB (778204071 B) → gzip 247.89 MB → 247771 examples\n",
      "✓ spa_Latn_data.jsonl → 4.25 GB (4561753718 B) → gzip 1.61 GB → 1375820 examples\n",
      "✓ ssw_Latn_data.jsonl → 12.35 MB (12945422 B) → gzip 4.25 MB → 2757 examples\n",
      "✓ swc_Latn_data.jsonl → 4.76 GB (5112611708 B) → gzip 1.69 GB → 2341117 examples\n",
      "✓ taq_Tfng_data.jsonl → 988.91 KB (1012648 B) → gzip 174.02 KB → 208 examples\n",
      "✓ tir_Ethi_data.jsonl → 469.35 MB (492151815 B) → gzip 134.03 MB → 82664 examples\n",
      "✓ tsn_Latn_data.jsonl → 242.82 MB (254616027 B) → gzip 73.76 MB → 162815 examples\n",
      "✓ tso_Latn_data.jsonl → 37.62 MB (39444757 B) → gzip 12.18 MB → 5198 examples\n",
      "✓ twi_Latn_data.jsonl → 29.45 MB (30884640 B) → gzip 10.97 MB → 5663 examples\n",
      "✓ tzm_Tfng_data.jsonl → 10.40 MB (10909955 B) → gzip 2.18 MB → 2455 examples\n",
      "✓ umb_Latn_data.jsonl → 1.44 MB (1506775 B) → gzip 476.81 KB → 709 examples\n",
      "✓ wol_Latn_data.jsonl → 4.55 MB (4766606 B) → gzip 1.72 MB → 879 examples\n",
      "✓ xho_Latn_data.jsonl → 693.13 MB (726801850 B) → gzip 253.45 MB → 222960 examples\n",
      "✓ yor_Latn_data.jsonl → 677.47 MB (710380488 B) → gzip 220.71 MB → 260843 examples\n",
      "✓ zul_Latn_data.jsonl → 927.12 MB (972160090 B) → gzip 337.53 MB → 336880 examples\n",
      "\n",
      "📊 Comparison saved as: jsonl_file_size_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# ─── Configuration ────────────────────────────────────────────────────────────\n",
    "base_dir   = \"/home/mila/x/xut/scratch/data/data_pretrain/llama_factory_data\"\n",
    "output_csv = \"jsonl_file_size_comparison.csv\"\n",
    "MAX_SIZE_BYTES = 2 * 1024 * 1024 * 1024  # 2 GB\n",
    "\n",
    "# ─── Helpers ───────────────────────────────────────────────────────────────────\n",
    "def get_readable_size(bytesize):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytesize < 1024:\n",
    "            return f\"{bytesize:.2f} {unit}\"\n",
    "        bytesize /= 1024\n",
    "    return f\"{bytesize:.2f} PB\"\n",
    "\n",
    "class _ByteCounter(io.RawIOBase):\n",
    "    \"\"\"A write-only stream that just counts bytes written to it.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "    def write(self, b):\n",
    "        n = len(b)\n",
    "        self.count += n\n",
    "        return n\n",
    "    def tell(self):\n",
    "        return self.count\n",
    "\n",
    "def get_gzip_size(filepath):\n",
    "    \"\"\"Stream-compress the file and return the total compressed byte count.\"\"\"\n",
    "    counter = _ByteCounter()\n",
    "    with open(filepath, 'rb') as f_in, \\\n",
    "         gzip.GzipFile(fileobj=counter, mode='wb') as gz:\n",
    "        # copyfileobj reads & writes in chunks, never storing all compressed data in RAM\n",
    "        shutil.copyfileobj(f_in, gz, length=1024 * 1024)\n",
    "    return counter.tell()\n",
    "\n",
    "def count_jsonl_lines(filepath):\n",
    "    \"\"\"Count the number of lines (examples) in a JSONL file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "# ─── Main Loop ─────────────────────────────────────────────────────────────────\n",
    "rows = []\n",
    "for filename in sorted(os.listdir(base_dir)):\n",
    "    if not filename.endswith(\".jsonl\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    try:\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        num_lines = count_jsonl_lines(filepath)\n",
    "        size_gzip_bytes = get_gzip_size(filepath)\n",
    "\n",
    "        rows.append([\n",
    "            filename,\n",
    "            get_readable_size(size_bytes),\n",
    "            size_bytes,\n",
    "            get_readable_size(size_gzip_bytes),\n",
    "            size_gzip_bytes,\n",
    "            f\"{(size_gzip_bytes / size_bytes * 100):.1f}%\",\n",
    "            num_lines\n",
    "        ])\n",
    "\n",
    "        print(\n",
    "            f\"✓ {filename} → \"\n",
    "            f\"{get_readable_size(size_bytes)} ({size_bytes} B) → \"\n",
    "            f\"gzip {get_readable_size(size_gzip_bytes)} → \"\n",
    "            f\"{num_lines} examples\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {filepath}: {e}\")\n",
    "\n",
    "# ─── Write to CSV ───────────────────────────────────────────────────────────────\n",
    "with open(output_csv, mode=\"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"File\",\n",
    "        \"Size (Human)\",\n",
    "        \"Size (Bytes)\",\n",
    "        \"Gzipped Size (Human)\",\n",
    "        \"Gzipped Size (Bytes)\",\n",
    "        \"Compression Ratio\",\n",
    "        \"Number of Examples\"\n",
    "    ])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"\\n📊 Comparison saved as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2306a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
